{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc62158e-d44a-4821-ad53-8cfbac025a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import bioimage\n",
    "\n",
    "from interactive_m2unet import M2UnetInteractiveModel\n",
    "import numpy as np\n",
    "import imageio\n",
    "import albumentations as A\n",
    "from skimage.filters import threshold_otsu\n",
    "from skimage.measure import label\n",
    "# Uncomment to specify the gpu number\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = \"1\"\n",
    "import torch\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "TRAIN = True\n",
    "TEST = False\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e47621-7cc7-4e3d-8e3a-7d3e87772581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function for loading cellpose output (image, mask and outline)\n",
    "def load_samples(train_dir):\n",
    "    npy_files = [os.path.join(train_dir, s) for s in os.listdir(train_dir) if s.endswith('.npy')]\n",
    "    samples = []\n",
    "    for file in npy_files:\n",
    "        print(file)\n",
    "        try:\n",
    "            items = np.load(file, allow_pickle=True).item()\n",
    "        except:\n",
    "            print(\"Bad Item\")\n",
    "            continue\n",
    "        mask = (items['masks'][:, :, None]  > 0) * 1.0\n",
    "        outline = (items['outlines'][:, :, None]  > 0) * 1.0\n",
    "        mask = mask * (1.0 - outline)\n",
    "        sample = (items['img'], mask)\n",
    "        samples.append(sample)\n",
    "    return samples\n",
    "\n",
    "# check if GPU is available\n",
    "print(f'GPU: {torch.cuda.is_available()}')\n",
    "\n",
    "# setting up\n",
    "data_dir = './cell_data_3' # data should contain a train and a test folder\n",
    "model_root = \"./models_100\"\n",
    "epochs = 2\n",
    "steps = 1\n",
    "resume = True\n",
    "corrid = \"200\"\n",
    "pretrained_model = None  # os.path.join(model_root, str(corrid), \"model.h5\")\n",
    "os.makedirs(os.path.join(model_root, str(corrid)), exist_ok=True)\n",
    "sz = 2048\n",
    "sz_outer = int(sz* 1.5)\n",
    "\n",
    "# define the transforms\n",
    "transform = A.Compose(\n",
    "    [\n",
    "        A.RandomCrop(sz, sz),\n",
    "        #A.Rotate(limit=[-5, 5], p=1),\n",
    "        A.Flip(p=0.5),\n",
    "        #A.CenterCrop(sz, sz),\n",
    "    ]\n",
    ")\n",
    "A.save(transform, \"./models/transform.json\")\n",
    "# unet model hyperparamer can be found here: https://notebooks.githubusercontent.com/view/ipynb?browser=chrome&color_mode=auto&commit=f899f7a8a9144b3f946c4a1362f7e38ae0c00c59&device=unknown_device&enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f79696e676b61697368612f6b657261732d756e65742d636f6c6c656374696f6e2f663839396637613861393134346233663934366334613133363266376533386165306330306335392f6578616d706c65732f757365725f67756964655f6d6f64656c732e6970796e62&logged_in=true&nwo=yingkaisha%2Fkeras-unet-collection&path=examples%2Fuser_guide_models.ipynb&platform=mac&repository_id=323426984&repository_type=Repository&version=95#Swin-UNET\n",
    "model_config = {\n",
    "    \"type\": \"m2unet\",\n",
    "    \"activation\": \"sigmoid\",\n",
    "    \"output_channels\": 1,\n",
    "    \"loss\": {\"name\": \"BCELoss\", \"kwargs\": {}},\n",
    "    \"optimizer\": {\"name\": \"RMSprop\", \"kwargs\": {\"lr\": 1e-2, \"weight_decay\": 1e-8, \"momentum\": 0.9}},\n",
    "    \"augmentation\": A.to_dict(transform),\n",
    "}\n",
    "model = M2UnetInteractiveModel(\n",
    "    model_config=model_config,\n",
    "    model_dir=model_root,\n",
    "    resume=resume,\n",
    "    pretrained_model=pretrained_model,\n",
    "    default_save_path=os.path.join(model_root, str(corrid), \"model.pth\"),\n",
    ")\n",
    "\n",
    "# load samples\n",
    "train_samples = load_samples(data_dir + '/train')\n",
    "test_samples = load_samples(data_dir + '/test')\n",
    "\n",
    "# train the model \n",
    "if TRAIN:\n",
    "    iterations = 0\n",
    "    for epoch in range(epochs):\n",
    "        print('epoch: ')\n",
    "        print(epoch)\n",
    "        losses = []\n",
    "        # image shape: sz, sz, 3\n",
    "        # labels shape: sz, sz, 1\n",
    "        for (image, labels) in train_samples:\n",
    "            mask = model.transform_labels(labels)\n",
    "            x = np.expand_dims(image, axis=0)\n",
    "            y = np.expand_dims(mask, axis=0)\n",
    "            losses = []\n",
    "            for _ in range(steps):\n",
    "                # x and y will be augmented for each step\n",
    "                loss = model.train_on_batch(x, y)\n",
    "                losses.append(loss)\n",
    "                iterations += 1\n",
    "                print(f\"iteration: {iterations}, loss: {loss}\")\n",
    "    model.save()\n",
    "\n",
    "# test\n",
    "if TEST:\n",
    "    for i, sample in enumerate(test_samples):\n",
    "        inputs = sample[0].astype(\"float32\")[None, :sz, :sz, :]\n",
    "        imageio.imwrite(f\"octopi-inputs_{i}.png\", inputs[0].astype('uint8'))\n",
    "        labels = sample[1].astype(\"float32\")[None, :sz, :sz, :] * 255\n",
    "        imageio.imwrite(f\"octopi-labels_{i}.png\", labels[0].astype('uint8'))\n",
    "        results = model.predict(inputs)\n",
    "        output = np.clip(results[0] * 255, 0, 255)[:, :, 0].astype('uint8')\n",
    "        imageio.imwrite(f\"octopi-pred-prob_{i}.png\", output)\n",
    "        threshold = threshold_otsu(output)\n",
    "        mask = ((output > threshold) * 255).astype('uint8')\n",
    "        predict_labels = label(mask)\n",
    "        imageio.imwrite(f\"octopi-pred-labels_{i}.png\", predict_labels)\n",
    "\n",
    "    print(\"all done\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a529bfb-8b30-420f-b995-ae16b4b80956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "TEST = True\n",
    "TRAIN = True\n",
    "\n",
    "import numpy as np\n",
    "import imageio\n",
    "import albumentations as A\n",
    "from skimage.filters import threshold_otsu\n",
    "from skimage.measure import label\n",
    "# Uncomment to specify the gpu number\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"1\"\n",
    "import torch\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# a function for loading cellpose output (image, mask and outline)\n",
    "def load_samples(train_dir):\n",
    "    npy_files = [os.path.join(train_dir, s) for s in os.listdir(train_dir) if s.endswith('.npy')]\n",
    "    samples = []\n",
    "    for file in npy_files:\n",
    "        print(file)\n",
    "        try:\n",
    "            items = np.load(file, allow_pickle=True).item()\n",
    "        except:\n",
    "            print(\"Bad Item\")\n",
    "            continue\n",
    "        mask = (items['masks'][:, :, None]  > 0) * 1.0\n",
    "        outline = (items['outlines'][:, :, None]  > 0) * 1.0\n",
    "        mask = mask * (1.0 - outline)\n",
    "        sample = (items['img'], mask)\n",
    "        samples.append(sample)\n",
    "    return samples\n",
    "\n",
    "# check if GPU is available\n",
    "print(f'GPU: {torch.cuda.is_available()}')\n",
    "\n",
    "# setting up\n",
    "data_dir = './cell_data_3' # data should contain a train and a test folder\n",
    "model_root = \"./models_100\"\n",
    "epochs = 1\n",
    "steps = 1\n",
    "resume = True\n",
    "corrid = \"200\"\n",
    "pretrained_model = None  # os.path.join(model_root, str(corrid), \"model.h5\")\n",
    "os.makedirs(os.path.join(model_root, str(corrid)), exist_ok=True)\n",
    "sz = 2048\n",
    "sz_outer = int(sz* 1.5)\n",
    "\n",
    "# define the transforms\n",
    "transform = A.Compose(\n",
    "    [\n",
    "        A.RandomCrop(sz, sz),\n",
    "        #A.Rotate(limit=[-5, 5], p=1),\n",
    "        A.Flip(p=0.5),\n",
    "        #A.CenterCrop(sz, sz),\n",
    "    ]\n",
    ")\n",
    "A.save(transform, \"./models/transform.json\")\n",
    "# unet model hyperparamer can be found here: https://notebooks.githubusercontent.com/view/ipynb?browser=chrome&color_mode=auto&commit=f899f7a8a9144b3f946c4a1362f7e38ae0c00c59&device=unknown_device&enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f79696e676b61697368612f6b657261732d756e65742d636f6c6c656374696f6e2f663839396637613861393134346233663934366334613133363266376533386165306330306335392f6578616d706c65732f757365725f67756964655f6d6f64656c732e6970796e62&logged_in=true&nwo=yingkaisha%2Fkeras-unet-collection&path=examples%2Fuser_guide_models.ipynb&platform=mac&repository_id=323426984&repository_type=Repository&version=95#Swin-UNET\n",
    "model_config = {\n",
    "    \"type\": \"m2unet\",\n",
    "    \"activation\": \"sigmoid\",\n",
    "    \"output_channels\": 1,\n",
    "    \"loss\": {\"name\": \"BCELoss\", \"kwargs\": {}},\n",
    "    \"optimizer\": {\"name\": \"RMSprop\", \"kwargs\": {\"lr\": 1e-2, \"weight_decay\": 1e-8, \"momentum\": 0.9}},\n",
    "    \"augmentation\": A.to_dict(transform),\n",
    "}\n",
    "model = M2UnetInteractiveModel(\n",
    "    model_config=model_config,\n",
    "    model_dir=model_root,\n",
    "    resume=resume,\n",
    "    pretrained_model=pretrained_model,\n",
    "    default_save_path=os.path.join(model_root, str(corrid), \"model.pth\"),\n",
    ")\n",
    "\n",
    "# load samples\n",
    "train_samples = load_samples(data_dir + '/train')\n",
    "test_samples = load_samples(data_dir + '/test')\n",
    "\n",
    "# train the model \n",
    "if TRAIN:\n",
    "    iterations = 0\n",
    "    for epoch in range(epochs):\n",
    "        print('epoch: ')\n",
    "        print(epoch)\n",
    "        losses = []\n",
    "        # image shape: sz, sz, 3\n",
    "        # labels shape: sz, sz, 1\n",
    "        for (image, labels) in train_samples:\n",
    "            mask = model.transform_labels(labels)\n",
    "            x = np.expand_dims(image, axis=0)\n",
    "            y = np.expand_dims(mask, axis=0)\n",
    "            losses = []\n",
    "            for _ in range(steps):\n",
    "                # x and y will be augmented for each step\n",
    "                loss = model.train_on_batch(x, y)\n",
    "                losses.append(loss)\n",
    "                iterations += 1\n",
    "                print(f\"iteration: {iterations}, loss: {loss}\")\n",
    "    model.save()\n",
    "\n",
    "# test\n",
    "if TEST:\n",
    "    for i, sample in enumerate(test_samples):\n",
    "        inputs = sample[0].astype(\"float32\")[None, :sz, :sz, :]\n",
    "        print[inputs[0].shape]\n",
    "        imageio.imwrite(f\"octopi-inputs_{i}.png\", inputs[0].astype('uint8'))\n",
    "        labels = sample[1].astype(\"float32\")[None, :sz, :sz, :] * 255\n",
    "        imageio.imwrite(f\"octopi-labels_{i}.png\", labels[0].astype('uint8'))\n",
    "        results = model.predict(inputs)\n",
    "        output = np.clip(results[0] * 255, 0, 255)[:, :, 0].astype('uint8')\n",
    "        imageio.imwrite(f\"octopi-pred-prob_{i}.png\", output)\n",
    "        threshold = threshold_otsu(output)\n",
    "        mask = ((output > threshold) * 255).astype('uint8')\n",
    "        predict_labels = label(mask)\n",
    "        imageio.imwrite(f\"octopi-pred-labels_{i}.png\", predict_labels)\n",
    "\n",
    "    print(\"all done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd856af-95ba-404b-9275-fa9c467f9a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.load('./072622-D5-6_2022-07-27_17-22-25.774065/3_0_f_BF_LED_matrix_dpc_seg.npy', allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eca23ce-cbc4-4604-b394-ef66fc39693e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae341128-934e-4a8a-a03b-626ca74f3ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jaccard Similarity\n",
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "\n",
    "labels_str = \"octopi-labels_\"\n",
    "pred_str = \"octopi-pred-labels_\"\n",
    "imgs = glob.glob(\"*.png\")\n",
    "n_labels = len([i for i in imgs if labels_str in i])\n",
    "\n",
    "def jaccard_sim(img1, img2):\n",
    "    n = np.prod(img1.shape)\n",
    "    a = img1 * img2\n",
    "    b = img1 + img2 - a\n",
    "    J = a/b\n",
    "    J[np.isnan(J)] = 1\n",
    "    j = np.sum(J)/n\n",
    "\n",
    "    return j\n",
    "\n",
    "\n",
    "j = []\n",
    "for i in range(n_labels):\n",
    "    label = labels_str + str(i) + \".png\"\n",
    "    pred  = pred_str + str(i) + \".png\"\n",
    "\n",
    "    i_pred = np.array(cv2.imread(pred)[:,:,0], dtype='f')\n",
    "    i_label = np.array(cv2.imread(label)[:,:,0], dtype='f')\n",
    "\n",
    "    i_pred = i_pred/255.0\n",
    "    i_label = i_label/255.0\n",
    "\n",
    "    j.append(jaccard_sim(i_label, i_pred))\n",
    "    \n",
    "print(j)\n",
    "print(min(j))\n",
    "print(max(j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9030c7ac-af2b-49b4-a470-d56b427c0f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Store Jaccard Similarity\n",
    "\n",
    "[0.6749610304832458, 0.6765353679656982, 0.6738048791885376, 0.6553453803062439, 0.6521809697151184, 0.6666283011436462, 0.6581857204437256, 0.6607326865196228, 0.6365719437599182, 0.6806446313858032, 0.6748928427696228, 0.6598854064941406, 0.6804280281066895, 0.6643669605255127, 0.6381222605705261, 0.6740015745162964, 0.6641574501991272, 0.6487610936164856, 0.663611650466919, 0.6803819537162781]\n",
    "0.6365719437599182\n",
    "0.6806446313858032\n",
    "\n",
    "\n",
    "[0.669924259185791, 0.6443573832511902, 0.6819045543670654, 0.6344529390335083, 0.6782583594322205, 0.6359854340553284, 0.6348254084587097, 0.6979055404663086, 0.6535060405731201, 0.7098541855812073, 0.6927616000175476, 0.6919836401939392, 0.6912431120872498, 0.6589328050613403, 0.678594708442688, 0.692324161529541, 0.662678062915802, 0.6622503995895386, 0.6850878000259399, 0.7019121050834656]\n",
    "0.6344529390335083\n",
    "0.7098541855812073"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu110.m95",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m95"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
