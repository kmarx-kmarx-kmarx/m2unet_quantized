{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d446acd8-7add-435f-9a86-0e7758f3d695",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The purpose of this file is to benchmark the speeds of different models before and after quantization\n",
    "'''\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51d37477-d1b6-4d8b-bcae-84fcedc8a02a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "def conv_bn(inp, oup, stride):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        nn.ReLU6(inplace=True)\n",
    "    )\n",
    "\n",
    "\n",
    "def conv_1x1_bn(inp, oup):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        nn.ReLU6(inplace=True)\n",
    "    )\n",
    "\n",
    "class InvertedResidual(nn.Module):\n",
    "    def __init__(self, inp, oup, stride, expand_ratio):\n",
    "        super(InvertedResidual, self).__init__()\n",
    "        self.stride = stride\n",
    "        assert stride in [1, 2]\n",
    "\n",
    "        hidden_dim = round(inp * expand_ratio)\n",
    "        self.use_res_connect = self.stride == 1 and inp == oup\n",
    "\n",
    "        if expand_ratio == 1:\n",
    "            # depthwise separable convolution block\n",
    "            self.conv = nn.Sequential(\n",
    "                # dw\n",
    "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.ReLU6(inplace=True),\n",
    "                # pw-linear\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "        else:\n",
    "            # Bottleneck with expansion layer\n",
    "            self.conv = nn.Sequential(\n",
    "                # pw\n",
    "                nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.ReLU6(inplace=True),\n",
    "                # dw\n",
    "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.ReLU6(inplace=True),\n",
    "                # pw-linear\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_res_connect:\n",
    "            return x + self.conv(x)\n",
    "        else:\n",
    "            return self.conv(x)\n",
    "        \n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    14 layers of MobileNetv2 as encoder part\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        block = InvertedResidual\n",
    "        interverted_residual_setting = [\n",
    "            # t, c, n, s\n",
    "            [1, 16, 1, 1],\n",
    "            [6, 24, 2, 2],\n",
    "            [6, 32, 3, 2],\n",
    "            [6, 64, 4, 2],\n",
    "            [6, 96, 3, 1],\n",
    "        ]\n",
    "        # Encoder Part\n",
    "        input_channel = 32 # number of input channels to first inverted (residual) block\n",
    "        self.layers = [conv_bn(3, 32, 2)]\n",
    "        # building inverted residual blocks\n",
    "        for t, c, n, s in interverted_residual_setting:\n",
    "            output_channel = c\n",
    "            for i in range(n):\n",
    "                if i == 0:\n",
    "                    self.layers.append(block(input_channel, output_channel, s, expand_ratio=t))\n",
    "                else:\n",
    "                    self.layers.append(block(input_channel, output_channel, 1, expand_ratio=t))\n",
    "                input_channel = output_channel\n",
    "        # make it nn.Sequential\n",
    "        self.layers = nn.Sequential(*self.layers)\n",
    "                \n",
    "class DecoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder block: upsample and concatenate with features maps from the encoder part\n",
    "    \"\"\"\n",
    "    def __init__(self,up_in_c,x_in_c,upsamplemode='bilinear',expand_ratio=0.15):\n",
    "        super().__init__()\n",
    "        self.upsample = nn.Upsample(scale_factor=2,mode=upsamplemode,align_corners=False) # H, W -> 2H, 2W\n",
    "        self.ir1 = InvertedResidual(up_in_c+x_in_c,(x_in_c + up_in_c) // 2,stride=1,expand_ratio=expand_ratio)\n",
    "\n",
    "    def forward(self,up_in,x_in):\n",
    "        up_out = self.upsample(up_in)\n",
    "        cat_x = torch.cat([up_out, x_in] , dim=1)\n",
    "        x = self.ir1(cat_x)\n",
    "        return x\n",
    "    \n",
    "class LastDecoderBlock(nn.Module):\n",
    "    def __init__(self,x_in_c,upsamplemode='bilinear',expand_ratio=0.15, output_channels=1, activation='linear'):\n",
    "        super().__init__()\n",
    "        self.upsample = nn.Upsample(scale_factor=2,mode=upsamplemode,align_corners=False) # H, W -> 2H, 2W\n",
    "        self.ir1 = InvertedResidual(x_in_c,16,stride=1,expand_ratio=expand_ratio)\n",
    "        layers =  [\n",
    "            nn.Conv2d(16, output_channels, 1, 1, 0, bias=True),\n",
    "        ]\n",
    "        if activation == 'sigmoid':\n",
    "            layers.append(nn.Sigmoid())\n",
    "        elif activation == 'softmax':\n",
    "            layers.append(nn.Softmax(dim=1))\n",
    "        elif activation == 'linear' or activation is None:\n",
    "            pass\n",
    "        else:\n",
    "            raise NotImplementedError('Activation {} not implemented'.format(activation))\n",
    "        self.conv = nn.Sequential(\n",
    "           *layers\n",
    "        )\n",
    "\n",
    "    def forward(self,up_in,x_in):\n",
    "        up_out = self.upsample(up_in)\n",
    "        cat_x = torch.cat([up_out, x_in] , dim=1)\n",
    "        x = self.ir1(cat_x)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "    \n",
    "class M2UNet(nn.Module):\n",
    "        def __init__(self,encoder,upsamplemode='bilinear',output_channels=1, activation=\"linear\", expand_ratio=0.15):\n",
    "            super(M2UNet,self).__init__()\n",
    "            encoder = list(encoder.children())[0]\n",
    "            # Encoder\n",
    "            self.conv1 = encoder[0:2]\n",
    "            self.conv2 = encoder[2:4]\n",
    "            self.conv3 = encoder[4:7]\n",
    "            self.conv4 = encoder[7:14]\n",
    "            # Decoder\n",
    "            self.decode4 = DecoderBlock(96,32,upsamplemode,expand_ratio)\n",
    "            self.decode3 = DecoderBlock(64,24,upsamplemode,expand_ratio)\n",
    "            self.decode2 = DecoderBlock(44,16,upsamplemode,expand_ratio)\n",
    "            self.decode1 = LastDecoderBlock(33,upsamplemode,expand_ratio, output_channels=output_channels, activation=activation)\n",
    "            # initilaize weights \n",
    "            self._init_params()\n",
    "\n",
    "        def _init_params(self):\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, nn.Conv2d):\n",
    "                    n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                    m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                    if m.bias is not None:\n",
    "                        m.bias.data.zero_()\n",
    "                elif isinstance(m, nn.BatchNorm2d):\n",
    "                    m.weight.data.fill_(1)\n",
    "                    m.bias.data.zero_()\n",
    "                elif isinstance(m, nn.Linear):\n",
    "                    n = m.weight.size(1)\n",
    "                    m.weight.data.normal_(0, 0.01)\n",
    "                    m.bias.data.zero_()\n",
    "            \n",
    "        \n",
    "        \n",
    "        def forward(self,x):\n",
    "            conv1 = self.conv1(x)\n",
    "            conv2 = self.conv2(conv1)\n",
    "            conv3 = self.conv3(conv2)\n",
    "            conv4 = self.conv4(conv3)\n",
    "            decode4 = self.decode4(conv4,conv3)\n",
    "            decode3 = self.decode3(decode4,conv2)\n",
    "            decode2 = self.decode2(decode3,conv1)\n",
    "            decode1 = self.decode1(decode2,x)\n",
    "            return decode1\n",
    "        \n",
    "def m2unet(output_channels=1,expand_ratio=0.15, activation=\"linear\", **kwargs):\n",
    "    encoder = Encoder()\n",
    "    model = M2UNet(encoder,upsamplemode='bilinear',expand_ratio=expand_ratio, output_channels=output_channels, activation=activation)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b41a4a8c-a382-4d9d-b571-99fbcb5f3d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "The average time per inference for unquantized m2unet is\n",
      "0.013181977272033692\n",
      "The average time per training cycle for unquantized m2unet is\n",
      "0.04086606502532959\n"
     ]
    }
   ],
   "source": [
    "## Define Model\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model = m2unet().float().to(device)\n",
    "\n",
    "\n",
    "## Inference speed\n",
    "trials = 100\n",
    "total_time = 0\n",
    "for i in range(trials):\n",
    "    inp = torch.randn([1, 3, 512, 512]).to(device)\n",
    "    torch.cuda.synchronize()\n",
    "    start_epoch = time.time()\n",
    "    output = model(inp)\n",
    "    torch.cuda.synchronize()\n",
    "    end_epoch = time.time()\n",
    "    elapsed = end_epoch - start_epoch\n",
    "    total_time += elapsed\n",
    "    \n",
    "print('The average time per inference for unquantized m2unet is')\n",
    "print(total_time/trials)\n",
    "\n",
    "\n",
    "## Training speed\n",
    "\n",
    "lr = .0001 # Doesn't matter\n",
    "lossfunc = nn.MSELoss().cuda('0')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "trials = 100\n",
    "total_time = 0\n",
    "batch_size = 1\n",
    "for i in range(trials):\n",
    "    torch.cuda.synchronize()\n",
    "                             \n",
    "    inp = torch.randn([batch_size, 3, 512, 512]).to(device)\n",
    "    label = torch.randn([batch_size, 1, 512, 512]).to(device)\n",
    "    start_epoch = time.time()\n",
    "                                                    \n",
    "    output = model(inp)                         \n",
    "    loss = lossfunc(output,label)\n",
    "    loss.backward()\n",
    "    optimizer.step()              \n",
    "                             \n",
    "    torch.cuda.synchronize()\n",
    "    end_epoch = time.time()\n",
    "    elapsed = end_epoch - start_epoch\n",
    "    total_time += elapsed\n",
    "    \n",
    "print('The average time per training cycle for unquantized m2unet is')\n",
    "print(total_time/trials)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcaa7833-0841-4e6f-aa30-2ebca65642de",
   "metadata": {},
   "outputs": [],
   "source": [
    "train() # run all operations once for cuda warm-up\n",
    "torch.cuda.synchronize() # wait for warm-up to finish\n",
    "\n",
    "times = []\n",
    "for e in range(epochs):\n",
    "    torch.cuda.synchronize()\n",
    "    start_epoch = time.time()\n",
    "    train()\n",
    "    torch.cuda.synchronize()\n",
    "    end_epoch = time.time()\n",
    "    elapsed = end_epoch - start_epoch\n",
    "    times.append(elapsed)\n",
    "\n",
    "avg_time = sum(times)/epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82b2a65-773f-4f37-84ab-e4b7867e7b04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu110.m95",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m95"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
