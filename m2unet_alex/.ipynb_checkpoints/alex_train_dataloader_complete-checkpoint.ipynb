{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc62158e-d44a-4821-ad53-8cfbac025a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/conda/lib/python3.7/site-packages/bioimageio/spec/shared/_resolve_source.py:433: CacheWarning: found cached /tmp/jupyter/bioimageio_cache/https/raw.githubusercontent.com/bioimage-io/bioimage.io/main/site.config.json. Skipping download of https://raw.githubusercontent.com/bioimage-io/bioimage.io/main/site.config.json.\n",
      "  warnings.warn(f\"found cached {local_path}. Skipping download of {uri}.\", category=CacheWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/bioimageio/spec/shared/_resolve_source.py:433: CacheWarning: found cached /tmp/jupyter/bioimageio_cache/https/bioimage-io.github.io/collection-bioimage-io/collection.json. Skipping download of https://bioimage-io.github.io/collection-bioimage-io/collection.json.\n",
      "  warnings.warn(f\"found cached {local_path}. Skipping download of {uri}.\", category=CacheWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import bioimage\n",
    "\n",
    "from interactive_m2unet import M2UnetInteractiveModel\n",
    "import numpy as np\n",
    "import imageio\n",
    "import albumentations as A\n",
    "from skimage.filters import threshold_otsu\n",
    "from skimage.measure import label\n",
    "# Uncomment to specify the gpu number\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = \"1\"\n",
    "import torch\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "import torchvision\n",
    "from glob import glob\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transform\n",
    "from torchvision.utils import make_grid\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21a5313a-8f7c-48b8-bd2b-15efc5a19c56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function torch.cuda.memory.mem_get_info(device: Union[torch.device, str, NoneType, int] = None) -> int>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sz = 512\n",
    "torch.cuda.mem_get_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9dc090f2-38d6-4780-a552-788150a2dece",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Custom Dataset class.\n",
    "Randomly samples subsets of available data as a form of data augmentation to create more data, parameterized by sub_samples\n",
    "'''\n",
    "class MyDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, images_path ,transform_img=None ,transform_label=None, sub_samples = 1):\n",
    "        \n",
    "        self.images_path = images_path\n",
    "        self.transform_img = transform_img\n",
    "        self.transform_label = transform_label\n",
    "        self.sub_samples = sub_samples\n",
    "        self.files_list = [os.path.join(self.images_path, s) for s in os.listdir(self.images_path) if s.endswith('.npy')]\n",
    "\n",
    "    def __len__(self):\n",
    "                \n",
    "        \n",
    "        return len(self.files_list)*self.sub_samples\n",
    "        \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        # We use sub_samples to randomly sample large images to create more training data\n",
    "        # Create num sub_samples of each image\n",
    "        \n",
    "        i = math.floor(idx/self.sub_samples)\n",
    "        \n",
    "        file = self.files_list[i]\n",
    "        \n",
    "        print(file)\n",
    "        try:\n",
    "            items = np.load(file, allow_pickle=True).item()\n",
    "        except:\n",
    "            print(\"Bad Item\")\n",
    "            \n",
    "        mask = (items['masks'][:, :, None]  > 0) * 1\n",
    "        outline = (items['outlines'][:, :, None]  > 0) * 1\n",
    "        mask = mask * (1 - outline)\n",
    "        \n",
    "        # print(items['img'].type)\n",
    "        \n",
    "        # Create random seed for transforms\n",
    "        seed = np.random.randint(53434)\n",
    "        \n",
    "        # Use same same seed for label and image transform\n",
    "        if self.transform_img:\n",
    "            random.seed(seed)\n",
    "            image = self.transform_img(items['img'])\n",
    "            \n",
    "            random.seed(seed)\n",
    "            mask = np.uint8(mask)\n",
    "            label = self.transform_img(mask)\n",
    "            \n",
    "\n",
    "        return image, label\n",
    "    \n",
    "\n",
    "    \n",
    "mytransformsImage = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.RandomResizedCrop(sz),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "mytransformsLabel = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "        transforms.RandomResizedCrop(sz),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6483de55-3a13-44ad-92de-a4a5ba732014",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "data_dir = './cell_data_3'\n",
    "sub_samples = 2\n",
    "\n",
    "traindata = MyDataset(data_dir + '/train', mytransformsImage, mytransformsLabel, sub_samples)   \n",
    "train_dataloader = DataLoader(traindata,batch_size)\n",
    "\n",
    "testdata = MyDataset(data_dir + '/test', mytransformsImage, mytransformsLabel, sub_samples)   \n",
    "test_dataloader = DataLoader(traindata,batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f912fee-c276-45f9-9f35-c4c3e42708ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCheck data size\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Check data size\n",
    "'''\n",
    "\n",
    "# train_features, train_labels = next(iter(train_loader))\n",
    "# print(f\"Feature batch shape: {train_features.size()}\")\n",
    "# print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "# img = train_features[0].squeeze()\n",
    "# print(img.shape)\n",
    "# label = train_labels[0]\n",
    "# plt.imshow(img, cmap=\"gray\")\n",
    "# plt.show()\n",
    "# print(f\"Label: {label}\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0a480ef-16ce-42dd-9d3a-c563bd186fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from glob import glob\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transform\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "class Convblock(nn.Module):\n",
    "    \n",
    "      def __init__(self,input_channel,output_channel,kernal=3,stride=1,padding=1):\n",
    "            \n",
    "        super().__init__()\n",
    "        self.convblock = nn.Sequential(\n",
    "            nn.Conv2d(input_channel,output_channel,kernal,stride,padding),\n",
    "            nn.BatchNorm2d(output_channel),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(output_channel,output_channel,kernal),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "    \n",
    "\n",
    "      def forward(self,x):\n",
    "        x = self.convblock(x)\n",
    "        return x\n",
    "    \n",
    "class UNet(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_channel,retain=True):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = Convblock(input_channel,32)\n",
    "        self.conv2 = Convblock(32,64)\n",
    "        self.conv3 = Convblock(64,128)\n",
    "        self.conv4 = Convblock(128,256)\n",
    "        self.neck = nn.Conv2d(256,512,3,1)\n",
    "        self.upconv4 = nn.ConvTranspose2d(512,256,3,2,0,1)\n",
    "        self.dconv4 = Convblock(512,256)\n",
    "        self.upconv3 = nn.ConvTranspose2d(256,128,3,2,0,1)\n",
    "        self.dconv3 = Convblock(256,128)\n",
    "        self.upconv2 = nn.ConvTranspose2d(128,64,3,2,0,1)\n",
    "        self.dconv2 = Convblock(128,64)\n",
    "        self.upconv1 = nn.ConvTranspose2d(64,32,3,2,0,1)\n",
    "        self.dconv1 = Convblock(64,32)\n",
    "        self.out = nn.Conv2d(32,1,1,1)\n",
    "        self.retain = retain\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        # Encoder Network\n",
    "        \n",
    "        # Conv down 1\n",
    "        conv1 = self.conv1(x)\n",
    "        pool1 = F.max_pool2d(conv1,kernel_size=2,stride=2)\n",
    "        # Conv down 2\n",
    "        conv2 = self.conv2(pool1)\n",
    "        pool2 = F.max_pool2d(conv2,kernel_size=2,stride=2)\n",
    "        # Conv down 3\n",
    "        conv3 = self.conv3(pool2)\n",
    "        pool3 = F.max_pool2d(conv3,kernel_size=2,stride=2)\n",
    "        # Conv down 4\n",
    "        conv4 = self.conv4(pool3)\n",
    "        pool4 = F.max_pool2d(conv4,kernel_size=2,stride=2)\n",
    "\n",
    "        # BottelNeck\n",
    "        neck = self.neck(pool4)\n",
    "        \n",
    "        # Decoder Network\n",
    "        \n",
    "        # Upconv 1\n",
    "        upconv4 = self.upconv4(neck)\n",
    "        croped = self.crop(conv4,upconv4)\n",
    "        # Making the skip connection 1\n",
    "        dconv4 = self.dconv4(torch.cat([upconv4,croped],1))\n",
    "        # Upconv 2\n",
    "        upconv3 = self.upconv3(dconv4)\n",
    "        croped = self.crop(conv3,upconv3)\n",
    "        # Making the skip connection 2\n",
    "        dconv3 = self.dconv3(torch.cat([upconv3,croped],1))\n",
    "        # Upconv 3\n",
    "        upconv2 = self.upconv2(dconv3)\n",
    "        croped = self.crop(conv2,upconv2)\n",
    "        # Making the skip connection 3\n",
    "        dconv2 = self.dconv2(torch.cat([upconv2,croped],1))\n",
    "        # Upconv 4\n",
    "        upconv1 = self.upconv1(dconv2)\n",
    "        croped = self.crop(conv1,upconv1)\n",
    "        # Making the skip connection 4\n",
    "        dconv1 = self.dconv1(torch.cat([upconv1,croped],1))\n",
    "        # Output Layer\n",
    "        out = self.out(dconv1)\n",
    "        \n",
    "        if self.retain == True:\n",
    "            out = F.interpolate(out,list(x.shape)[2:])\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def crop(self,input_tensor,target_tensor):\n",
    "        # For making the size of the encoder conv layer and the decoder Conv layer same\n",
    "        _,_,H,W = target_tensor.shape\n",
    "        return transform.CenterCrop([H,W])(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f3a8a12-f77b-4508-811f-37d2d1ac3613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# initializing the model\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model = UNet(3).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b8b594b-9415-4556-ace7-18e166cea4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./cell_data_3/train/0_6_f_BF_LED_matrix_dpc_seg.npy\n",
      "./cell_data_3/train/0_6_f_BF_LED_matrix_dpc_seg.npy\n",
      "./cell_data_3/train/5_9_f_BF_LED_matrix_dpc_seg.npy\n",
      "./cell_data_3/train/5_9_f_BF_LED_matrix_dpc_seg.npy\n",
      "./cell_data_3/train/2_3_f_BF_LED_matrix_dpc_seg.npy\n",
      "./cell_data_3/train/2_3_f_BF_LED_matrix_dpc_seg.npy\n",
      "./cell_data_3/train/0_9_f_BF_LED_matrix_dpc_seg.npy\n",
      "./cell_data_3/train/0_9_f_BF_LED_matrix_dpc_seg.npy\n",
      "./cell_data_3/train/1_4_f_BF_LED_matrix_dpc_seg.npy\n",
      "./cell_data_3/train/1_4_f_BF_LED_matrix_dpc_seg.npy\n",
      "./cell_data_3/train/6_4_f_BF_LED_matrix_dpc_seg.npy\n",
      "./cell_data_3/train/6_4_f_BF_LED_matrix_dpc_seg.npy\n",
      "./cell_data_3/train/0_2_f_BF_LED_matrix_dpc_seg.npy\n",
      "./cell_data_3/train/0_2_f_BF_LED_matrix_dpc_seg.npy\n",
      "./cell_data_3/train/8_2_f_BF_LED_matrix_dpc_seg.npy\n",
      "./cell_data_3/train/8_2_f_BF_LED_matrix_dpc_seg.npy\n",
      "./cell_data_3/train/8_3_f_BF_LED_matrix_dpc_seg.npy\n",
      "./cell_data_3/train/8_3_f_BF_LED_matrix_dpc_seg.npy\n",
      "./cell_data_3/train/2_6_f_BF_LED_matrix_dpc_seg.npy\n",
      "./cell_data_3/train/2_6_f_BF_LED_matrix_dpc_seg.npy\n",
      "./cell_data_3/train/9_8_f_BF_LED_matrix_dpc_seg.npy\n",
      "./cell_data_3/train/9_8_f_BF_LED_matrix_dpc_seg.npy\n",
      "./cell_data_3/train/6_2_f_BF_LED_matrix_dpc_seg.npy\n",
      "./cell_data_3/train/6_2_f_BF_LED_matrix_dpc_seg.npy\n",
      "./cell_data_3/train/5_7_f_BF_LED_matrix_dpc_seg.npy\n",
      "./cell_data_3/train/5_7_f_BF_LED_matrix_dpc_seg.npy\n",
      "./cell_data_3/train/0_7_f_BF_LED_matrix_dpc_seg.npy\n",
      "./cell_data_3/train/0_7_f_BF_LED_matrix_dpc_seg.npy\n",
      "./cell_data_3/train/1_5_f_BF_LED_matrix_dpc_seg.npy\n",
      "./cell_data_3/train/1_5_f_BF_LED_matrix_dpc_seg.npy\n",
      "./cell_data_3/train/2_0_f_BF_LED_matrix_dpc_seg.npy\n",
      "./cell_data_3/train/2_0_f_BF_LED_matrix_dpc_seg.npy\n",
      "./cell_data_3/train/9_4_f_BF_LED_matrix_dpc_seg.npy\n",
      "./cell_data_3/train/9_4_f_BF_LED_matrix_dpc_seg.npy\n",
      "./cell_data_3/train/5_0_f_BF_LED_matrix_dpc_seg.npy\n",
      "./cell_data_3/train/5_0_f_BF_LED_matrix_dpc_seg.npy\n",
      "./cell_data_3/train/7_4_f_BF_LED_matrix_dpc_seg.npy\n",
      "./cell_data_3/train/7_4_f_BF_LED_matrix_dpc_seg.npy\n",
      "./cell_data_3/train/2_4_f_BF_LED_matrix_dpc_seg.npy\n",
      "./cell_data_3/train/2_4_f_BF_LED_matrix_dpc_seg.npy\n",
      "./cell_data_3/train/9_9_f_BF_LED_matrix_dpc_seg.npy\n",
      "./cell_data_3/train/9_9_f_BF_LED_matrix_dpc_seg.npy\n",
      "./cell_data_3/train/7_6_f_BF_LED_matrix_dpc_seg.npy\n",
      "./cell_data_3/train/7_6_f_BF_LED_matrix_dpc_seg.npy\n",
      "./cell_data_3/train/7_0_f_BF_LED_matrix_dpc_seg.npy\n",
      "./cell_data_3/train/7_0_f_BF_LED_matrix_dpc_seg.npy\n",
      "./cell_data_3/train/8_0_f_BF_LED_matrix_dpc_seg.npy\n",
      "./cell_data_3/train/8_0_f_BF_LED_matrix_dpc_seg.npy\n",
      "./cell_data_3/train/1_0_f_BF_LED_matrix_dpc_seg.npy\n",
      "./cell_data_3/train/1_0_f_BF_LED_matrix_dpc_seg.npy\n",
      "./cell_data_3/train/8_4_f_BF_LED_matrix_dpc_seg.npy\n",
      "./cell_data_3/train/8_4_f_BF_LED_matrix_dpc_seg.npy\n",
      "./cell_data_3/train/5_4_f_BF_LED_matrix_dpc_seg.npy\n",
      "./cell_data_3/train/5_4_f_BF_LED_matrix_dpc_seg.npy\n",
      "./cell_data_3/train/0_4_f_BF_LED_matrix_dpc_seg.npy\n",
      "./cell_data_3/train/0_4_f_BF_LED_matrix_dpc_seg.npy\n",
      "./cell_data_3/train/0_3_f_BF_LED_matrix_dpc_seg.npy\n",
      "./cell_data_3/train/0_3_f_BF_LED_matrix_dpc_seg.npy\n",
      "./cell_data_3/train/2_5_f_BF_LED_matrix_dpc_seg.npy\n",
      "./cell_data_3/train/2_5_f_BF_LED_matrix_dpc_seg.npy\n",
      "./cell_data_3/train/7_1_f_BF_LED_matrix_dpc_seg.npy\n",
      "./cell_data_3/train/7_1_f_BF_LED_matrix_dpc_seg.npy\n",
      "./cell_data_3/train/2_8_f_BF_LED_matrix_dpc_seg.npy\n",
      "./cell_data_3/train/2_8_f_BF_LED_matrix_dpc_seg.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:24<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 1.55 GiB (GPU 0; 39.41 GiB total capacity; 27.06 GiB already allocated; 853.50 MiB free; 28.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13492/3114950661.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m256\u001b[0m \u001b[0;31m# pushing data between 0 and 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlossfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_13492/1175488912.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mcroped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mupconv1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;31m# Making the skip connection 4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0mdconv1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mupconv1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcroped\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         \u001b[0;31m# Output Layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdconv1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_13492/1175488912.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    453\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 454\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 1.55 GiB (GPU 0; 39.41 GiB total capacity; 27.06 GiB already allocated; 853.50 MiB free; 28.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# device = 'cuda:0'\n",
    "lr = 0.01\n",
    "epochs = 300\n",
    "lossfunc = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "train_acc = []\n",
    "val_acc = []\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "\n",
    "for i in range(epochs):\n",
    "    \n",
    "    trainloss = 0\n",
    "    valloss = 0\n",
    "    \n",
    "    for img,label in tqdm(train_dataloader):\n",
    "        '''\n",
    "            Traning the Model.\n",
    "        '''\n",
    "        optimizer.zero_grad()\n",
    "        img = img.float().to(device)\n",
    "        img = img / 256 # pushing data between 0 and 1\n",
    "        label = label.float().to(device)\n",
    "        output = model(img)\n",
    "        loss = lossfunc(output,label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        trainloss+=loss.item()\n",
    "    \n",
    "    # if(i%5==0):\n",
    "    #     show(img,output,label)\n",
    "\n",
    "    train_loss.append(trainloss/len(train_dataloader))    \n",
    "    \n",
    "  \n",
    "    for img,label in tqdm(test_dataloader):\n",
    "        '''\n",
    "            Validation of Model.\n",
    "        '''\n",
    "        img = img.float().to(device)\n",
    "        img = img / 256 # pushing data between 0 and 1\n",
    "\n",
    "        label = label.float().to(device)\n",
    "        output = model(img)\n",
    "        loss = lossfunc(output,label)\n",
    "        valloss+=loss.item()\n",
    "        \n",
    "    val_loss.append(valloss/len(test_dataloader))  \n",
    "    \n",
    "    \n",
    "    print(\"epoch : {} ,train loss : {} ,valid loss : {} \".format(i,train_loss[-1],val_loss[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85afcea2-c88c-4da8-ad4f-bc3d9899d9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot sample data\n",
    "def show(img,output,label,denorm = False):\n",
    "    img,output,label = img.cpu(),output.cpu(),label.cpu()\n",
    "    fig,ax = plt.subplots(len(output),3,figsize=(15,30))\n",
    "    cols = ['Input Image','Actual Output','Predicted Output']\n",
    "    for i in range(len(output)):\n",
    "        if(len(output) == 3):\n",
    "            Img,Lab,act = img[i],output[i],label[i]\n",
    "            Img,Lab,act = Img,Lab.detach().permute(1,2,0).numpy(),act\n",
    "            ax[i][0].imshow(Img.permute(1,2,0))\n",
    "            ax[i][2].imshow(Lab)\n",
    "            ax[i][1].imshow(act.permute(1,2,0))\n",
    "        else:\n",
    "            Img,Lab,act = img[i],output[i],label[i]\n",
    "            Img,Lab,act = Img,Lab.detach().permute(1,2,0).numpy(),act\n",
    "            ax[0].imshow(Img.permute(1,2,0))\n",
    "            ax[2].imshow(Lab)\n",
    "            ax[1].imshow(act.permute(1,2,0))\n",
    "            #ax[0].title('this')\n",
    "            for ax, col in zip(ax, cols):\n",
    "                ax.set_title(col)\n",
    "    plt.show()\n",
    "    \n",
    "c = 0\n",
    "for img,label in (test_dataloader):\n",
    "        img = img.float().to(device)\n",
    "        img = img / 256\n",
    "        label = label.float().to(device)\n",
    "        output = model(img)\n",
    "        show(img,output,label)\n",
    "        if c>20:\n",
    "            break\n",
    "        c+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d63dd1c-aed4-4ff6-aaec-641fe9979e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a sample \n",
    "sz = 512\n",
    "\n",
    "\n",
    "Plot data sample\n",
    "\n",
    "\n",
    "\n",
    "from skimage.filters import threshold_otsu\n",
    "from skimage.measure import label\n",
    "for i, sample in enumerate(test_dataloader):\n",
    "    img, labels = sample\n",
    "    img = img.float().to(device)\n",
    "    labels = labels.float().to(device)\n",
    "    print(labels)\n",
    "    \n",
    "    input_temp = img.cpu().detach().numpy().astype(\"float32\")[None, :sz, :sz, :]\n",
    "    temp = np.swapaxes(input_temp[0,0,:,:], 0, 2)\n",
    "        \n",
    "    # Write input image\n",
    "    imageio.imwrite(f\"octopi-inputs_{i}.png\", temp.astype('uint8'))\n",
    "    \n",
    "    #Write label image\n",
    "    label_temp = labels.cpu().detach().numpy().astype(\"float32\")[None, :sz, :sz, :]\n",
    "    temp = np.swapaxes(label_temp[0,0,:,:], 0, 2)\n",
    "    print(temp.shape)\n",
    "    print(temp)\n",
    "\n",
    "    imageio.imwrite(f\"octopi-labels_{i}.png\", temp.astype('uint8'))\n",
    "    \n",
    "    # write predicted label image\n",
    "    results = model(img)\n",
    "    print(results.shape)\n",
    "    result_temp = results.cpu().detach().numpy()\n",
    "    print(result_temp.shape)\n",
    "    temp = result_temp[0,:,:,:]\n",
    "    print('result temp shape')\n",
    "    print(temp.shape)\n",
    "    \n",
    "    output = np.clip(temp * 255, 0, 255)[:, :, :].astype('uint8')\n",
    "\n",
    "    temp =  np.swapaxes(output, 0,2)\n",
    "    print(temp.shape)\n",
    "    imageio.imwrite(f\"octopi-pred-prob_{i}.png\", temp)\n",
    "    \n",
    "    print(np.mean(temp))\n",
    "    \n",
    "    threshold = threshold_otsu(temp)\n",
    "    mask = ((output > threshold) * 255).astype('uint8')\n",
    "    \n",
    "    \n",
    "    # print(mask.type)\n",
    "    print(mask.shape)\n",
    "    mask = np.swapaxes(mask, 0, 2)\n",
    "    predict_labels = label(mask)\n",
    "    imageio.imwrite(f\"octopi-pred-labels_{i}.png\", predict_labels)\n",
    "\n",
    "print(\"all done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e47621-7cc7-4e3d-8e3a-7d3e87772581",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# a function for loading cellpose output (image, mask and outline)\n",
    "\n",
    "\n",
    "# check if GPU is available\n",
    "print(f'GPU: {torch.cuda.is_available()}')\n",
    "\n",
    "# setting up\n",
    "data_dir = './cell_data_3' # data should contain a train and a test folder\n",
    "model_root = \"./models_100\"\n",
    "epochs = 2\n",
    "steps = 1\n",
    "resume = True\n",
    "corrid = \"200\"\n",
    "pretrained_model = None  # os.path.join(model_root, str(corrid), \"model.h5\")\n",
    "os.makedirs(os.path.join(model_root, str(corrid)), exist_ok=True)\n",
    "sz = 2048\n",
    "sz_outer = int(sz* 1.5)\n",
    "\n",
    "# define the transforms\n",
    "transform = A.Compose(\n",
    "    [\n",
    "        A.RandomCrop(sz, sz),\n",
    "        #A.Rotate(limit=[-5, 5], p=1),\n",
    "        A.Flip(p=0.5),\n",
    "        #A.CenterCrop(sz, sz),\n",
    "    ]\n",
    ")\n",
    "A.save(transform, \"./models/transform.json\")\n",
    "# unet model hyperparamer can be found here: https://notebooks.githubusercontent.com/view/ipynb?browser=chrome&color_mode=auto&commit=f899f7a8a9144b3f946c4a1362f7e38ae0c00c59&device=unknown_device&enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f79696e676b61697368612f6b657261732d756e65742d636f6c6c656374696f6e2f663839396637613861393134346233663934366334613133363266376533386165306330306335392f6578616d706c65732f757365725f67756964655f6d6f64656c732e6970796e62&logged_in=true&nwo=yingkaisha%2Fkeras-unet-collection&path=examples%2Fuser_guide_models.ipynb&platform=mac&repository_id=323426984&repository_type=Repository&version=95#Swin-UNET\n",
    "model_config = {\n",
    "    \"type\": \"m2unet\",\n",
    "    \"activation\": \"sigmoid\",\n",
    "    \"output_channels\": 1,\n",
    "    \"loss\": {\"name\": \"BCELoss\", \"kwargs\": {}},\n",
    "    \"optimizer\": {\"name\": \"RMSprop\", \"kwargs\": {\"lr\": 1e-2, \"weight_decay\": 1e-8, \"momentum\": 0.9}}\n",
    "    # \"augmentation\": A.to_dict(transform),\n",
    "}\n",
    "model = M2UnetInteractiveModel(\n",
    "    model_config=model_config,\n",
    "    model_dir=model_root,\n",
    "    resume=resume,\n",
    "    pretrained_model=pretrained_model,\n",
    "    default_save_path=os.path.join(model_root, str(corrid), \"model.pth\"),\n",
    ")\n",
    "\n",
    "# # load samples\n",
    "# train_samples = load_samples(data_dir + '/train')\n",
    "# test_samples = load_samples(data_dir + '/test')\n",
    "\n",
    "# train the model \n",
    "if TRAIN:\n",
    "    iterations = 0\n",
    "    for epoch in range(epochs):\n",
    "        print('epoch: ')\n",
    "        print(epoch)\n",
    "        losses = []\n",
    "        # image shape: sz, sz, 3\n",
    "        # labels shape: sz, sz, 1\n",
    "        for i, sample in enumerate(train_dataloader, 0):\n",
    "            image, labels = sample\n",
    "            mask = model.transform_labels(labels)\n",
    "            x = np.expand_dims(image, axis=1)\n",
    "            print(x.shape)\n",
    "            y = np.expand_dims(mask, axis=1)\n",
    "            print(x)\n",
    "            print(y)\n",
    "            losses = []\n",
    "            for _ in range(steps):\n",
    "                # x and y will be augmented for each step\n",
    "                loss = model.train_on_batch(x, y)\n",
    "                losses.append(loss)\n",
    "                iterations += 1\n",
    "                print(f\"iteration: {iterations}, loss: {loss}\")\n",
    "    model.save()\n",
    "\n",
    "# test\n",
    "if TEST:\n",
    "    for i, sample in enumerate(test_samples):\n",
    "        inputs = sample[0].astype(\"float32\")[None, :sz, :sz, :]\n",
    "        imageio.imwrite(f\"octopi-inputs_{i}.png\", inputs[0].astype('uint8'))\n",
    "        labels = sample[1].astype(\"float32\")[None, :sz, :sz, :] * 255\n",
    "        imageio.imwrite(f\"octopi-labels_{i}.png\", labels[0].astype('uint8'))\n",
    "        results = model.predict(inputs)\n",
    "        output = np.clip(results[0] * 255, 0, 255)[:, :, 0].astype('uint8')\n",
    "        imageio.imwrite(f\"octopi-pred-prob_{i}.png\", output)\n",
    "        threshold = threshold_otsu(output)\n",
    "        mask = ((output > threshold) * 255).astype('uint8')\n",
    "        predict_labels = label(mask)\n",
    "        imageio.imwrite(f\"octopi-pred-labels_{i}.png\", predict_labels)\n",
    "\n",
    "    print(\"all done\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a529bfb-8b30-420f-b995-ae16b4b80956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "TEST = True\n",
    "TRAIN = True\n",
    "\n",
    "import numpy as np\n",
    "import imageio\n",
    "import albumentations as A\n",
    "from skimage.filters import threshold_otsu\n",
    "from skimage.measure import label\n",
    "# Uncomment to specify the gpu number\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"1\"\n",
    "import torch\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# a function for loading cellpose output (image, mask and outline)\n",
    "def load_samples(train_dir):\n",
    "    npy_files = [os.path.join(train_dir, s) for s in os.listdir(train_dir) if s.endswith('.npy')]\n",
    "    samples = []\n",
    "    for file in npy_files:\n",
    "        print(file)\n",
    "        try:\n",
    "            items = np.load(file, allow_pickle=True).item()\n",
    "        except:\n",
    "            print(\"Bad Item\")\n",
    "            continue\n",
    "        mask = (items['masks'][:, :, None]  > 0) * 1.0\n",
    "        outline = (items['outlines'][:, :, None]  > 0) * 1.0\n",
    "        mask = mask * (1.0 - outline)\n",
    "        sample = (items['img'], mask)\n",
    "        samples.append(sample)\n",
    "    return samples\n",
    "\n",
    "# check if GPU is available\n",
    "print(f'GPU: {torch.cuda.is_available()}')\n",
    "\n",
    "# setting up\n",
    "data_dir = './cell_data_3' # data should contain a train and a test folder\n",
    "model_root = \"./models_100\"\n",
    "epochs = 1\n",
    "steps = 1\n",
    "resume = True\n",
    "corrid = \"200\"\n",
    "pretrained_model = None  # os.path.join(model_root, str(corrid), \"model.h5\")\n",
    "os.makedirs(os.path.join(model_root, str(corrid)), exist_ok=True)\n",
    "sz = 2048\n",
    "sz_outer = int(sz* 1.5)\n",
    "\n",
    "# define the transforms\n",
    "transform = A.Compose(\n",
    "    [\n",
    "        A.RandomCrop(sz, sz),\n",
    "        #A.Rotate(limit=[-5, 5], p=1),\n",
    "        A.Flip(p=0.5),\n",
    "        #A.CenterCrop(sz, sz),\n",
    "    ]\n",
    ")\n",
    "A.save(transform, \"./models/transform.json\")\n",
    "# unet model hyperparamer can be found here: https://notebooks.githubusercontent.com/view/ipynb?browser=chrome&color_mode=auto&commit=f899f7a8a9144b3f946c4a1362f7e38ae0c00c59&device=unknown_device&enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f79696e676b61697368612f6b657261732d756e65742d636f6c6c656374696f6e2f663839396637613861393134346233663934366334613133363266376533386165306330306335392f6578616d706c65732f757365725f67756964655f6d6f64656c732e6970796e62&logged_in=true&nwo=yingkaisha%2Fkeras-unet-collection&path=examples%2Fuser_guide_models.ipynb&platform=mac&repository_id=323426984&repository_type=Repository&version=95#Swin-UNET\n",
    "model_config = {\n",
    "    \"type\": \"m2unet\",\n",
    "    \"activation\": \"sigmoid\",\n",
    "    \"output_channels\": 1,\n",
    "    \"loss\": {\"name\": \"BCELoss\", \"kwargs\": {}},\n",
    "    \"optimizer\": {\"name\": \"RMSprop\", \"kwargs\": {\"lr\": 1e-2, \"weight_decay\": 1e-8, \"momentum\": 0.9}},\n",
    "    \"augmentation\": A.to_dict(transform),\n",
    "}\n",
    "model = M2UnetInteractiveModel(\n",
    "    model_config=model_config,\n",
    "    model_dir=model_root,\n",
    "    resume=resume,\n",
    "    pretrained_model=pretrained_model,\n",
    "    default_save_path=os.path.join(model_root, str(corrid), \"model.pth\"),\n",
    ")\n",
    "\n",
    "# load samples\n",
    "train_samples = load_samples(data_dir + '/train')\n",
    "test_samples = load_samples(data_dir + '/test')\n",
    "\n",
    "# train the model \n",
    "if TRAIN:\n",
    "    iterations = 0\n",
    "    for epoch in range(epochs):\n",
    "        print('epoch: ')\n",
    "        print(epoch)\n",
    "        losses = []\n",
    "        # image shape: sz, sz, 3\n",
    "        # labels shape: sz, sz, 1\n",
    "        for (image, labels) in train_samples:\n",
    "            mask = model.transform_labels(labels)\n",
    "            x = np.expand_dims(image, axis=0)\n",
    "            y = np.expand_dims(mask, axis=0)\n",
    "            losses = []\n",
    "            for _ in range(steps):\n",
    "                # x and y will be augmented for each step\n",
    "                loss = model.train_on_batch(x, y)\n",
    "                losses.append(loss)\n",
    "                iterations += 1\n",
    "                print(f\"iteration: {iterations}, loss: {loss}\")\n",
    "    model.save()\n",
    "\n",
    "# test\n",
    "if TEST:\n",
    "    for i, sample in enumerate(test_samples):\n",
    "        inputs = sample[0].astype(\"float32\")[None, :sz, :sz, :]\n",
    "        imageio.imwrite(f\"octopi-inputs_{i}.png\", inputs[0].astype('uint8'))\n",
    "        labels = sample[1].astype(\"float32\")[None, :sz, :sz, :] * 255\n",
    "        imageio.imwrite(f\"octopi-labels_{i}.png\", labels[0].astype('uint8'))\n",
    "        results = model.predict(inputs)\n",
    "        output = np.clip(results[0] * 255, 0, 255)[:, :, 0].astype('uint8')\n",
    "        imageio.imwrite(f\"octopi-pred-prob_{i}.png\", output)\n",
    "        threshold = threshold_otsu(output)\n",
    "        mask = ((output > threshold) * 255).astype('uint8')\n",
    "        predict_labels = label(mask)\n",
    "        imageio.imwrite(f\"octopi-pred-labels_{i}.png\", predict_labels)\n",
    "\n",
    "    print(\"all done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd856af-95ba-404b-9275-fa9c467f9a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.load('./072622-D5-6_2022-07-27_17-22-25.774065/3_0_f_BF_LED_matrix_dpc_seg.npy', allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eca23ce-cbc4-4604-b394-ef66fc39693e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae341128-934e-4a8a-a03b-626ca74f3ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jaccard Similarity\n",
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "\n",
    "labels_str = \"octopi-labels_\"\n",
    "pred_str = \"octopi-pred-labels_\"\n",
    "imgs = glob.glob(\"*.png\")\n",
    "n_labels = len([i for i in imgs if labels_str in i])\n",
    "\n",
    "def jaccard_sim(img1, img2):\n",
    "    n = np.prod(img1.shape)\n",
    "    a = img1 * img2\n",
    "    b = img1 + img2 - a\n",
    "    J = a/b\n",
    "    J[np.isnan(J)] = 1\n",
    "    j = np.sum(J)/n\n",
    "\n",
    "    return j\n",
    "\n",
    "\n",
    "j = []\n",
    "for i in range(n_labels):\n",
    "    label = labels_str + str(i) + \".png\"\n",
    "    pred  = pred_str + str(i) + \".png\"\n",
    "\n",
    "    i_pred = np.array(cv2.imread(pred)[:,:,0], dtype='f')\n",
    "    i_label = np.array(cv2.imread(label)[:,:,0], dtype='f')\n",
    "\n",
    "    i_pred = i_pred/255.0\n",
    "    i_label = i_label/255.0\n",
    "\n",
    "    j.append(jaccard_sim(i_label, i_pred))\n",
    "    \n",
    "print(j)\n",
    "print(min(j))\n",
    "print(max(j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9030c7ac-af2b-49b4-a470-d56b427c0f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Store Jaccard Similarity\n",
    "\n",
    "[0.6749610304832458, 0.6765353679656982, 0.6738048791885376, 0.6553453803062439, 0.6521809697151184, 0.6666283011436462, 0.6581857204437256, 0.6607326865196228, 0.6365719437599182, 0.6806446313858032, 0.6748928427696228, 0.6598854064941406, 0.6804280281066895, 0.6643669605255127, 0.6381222605705261, 0.6740015745162964, 0.6641574501991272, 0.6487610936164856, 0.663611650466919, 0.6803819537162781]\n",
    "0.6365719437599182\n",
    "0.6806446313858032\n",
    "\n",
    "\n",
    "[0.669924259185791, 0.6443573832511902, 0.6819045543670654, 0.6344529390335083, 0.6782583594322205, 0.6359854340553284, 0.6348254084587097, 0.6979055404663086, 0.6535060405731201, 0.7098541855812073, 0.6927616000175476, 0.6919836401939392, 0.6912431120872498, 0.6589328050613403, 0.678594708442688, 0.692324161529541, 0.662678062915802, 0.6622503995895386, 0.6850878000259399, 0.7019121050834656]\n",
    "0.6344529390335083\n",
    "0.7098541855812073"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu110.m95",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m95"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
