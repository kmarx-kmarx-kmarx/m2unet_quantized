{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b990f96b-ff9b-408e-8791-d431c9dfd6b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nM2u-net implementation with tensorRT\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "M2u-net implementation with tensorRT\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fccf201c-5ff5-4580-9683-d6a73a9ca66c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/bioimageio/spec/shared/_resolve_source.py:433: CacheWarning: found cached /tmp/jupyter/bioimageio_cache/https/raw.githubusercontent.com/bioimage-io/bioimage.io/main/site.config.json. Skipping download of https://raw.githubusercontent.com/bioimage-io/bioimage.io/main/site.config.json.\n",
      "  warnings.warn(f\"found cached {local_path}. Skipping download of {uri}.\", category=CacheWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/bioimageio/spec/shared/_resolve_source.py:433: CacheWarning: found cached /tmp/jupyter/bioimageio_cache/https/bioimage-io.github.io/collection-bioimage-io/collection.json. Skipping download of https://bioimage-io.github.io/collection-bioimage-io/collection.json.\n",
      "  warnings.warn(f\"found cached {local_path}. Skipping download of {uri}.\", category=CacheWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
      "/opt/conda/lib/python3.7/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "import torch\n",
    "import torch.onnx\n",
    "import os\n",
    "import bioimage\n",
    "\n",
    "from interactive_m2unet import M2UnetInteractiveModel\n",
    "import numpy as np\n",
    "import imageio\n",
    "import albumentations as A\n",
    "from skimage.filters import threshold_otsu\n",
    "from skimage.measure import label\n",
    "# Uncomment to specify the gpu number\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = \"1\"\n",
    "import torch\n",
    "torch.backends.cudnn.benchmark = True\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from glob import glob\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transform\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torchvision.utils import make_grid\n",
    "import math\n",
    "\n",
    "\n",
    "# load the pretrained model\n",
    "resnet50 = models.resnet50(pretrained=True, progress=False).eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad1b1e2c-4a04-4a50-99bb-de760b5dd8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Modified m2unet for quantization\n",
    "'''\n",
    "\n",
    "def conv_bn(inp, oup, stride):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )\n",
    "\n",
    "\n",
    "def conv_1x1_bn(inp, oup):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )\n",
    "\n",
    "class InvertedResidual(nn.Module):\n",
    "    def __init__(self, inp, oup, stride, expand_ratio):\n",
    "        super(InvertedResidual, self).__init__()\n",
    "        self.stride = stride\n",
    "        self.add = nn.quantized.FloatFunctional()\n",
    "\n",
    "        \n",
    "        assert stride in [1, 2]\n",
    "        \n",
    "        # self.quant = torch.quantization.QuantStub()\n",
    "        # self.dequant = torch.quantization.DeQuantStub()\n",
    "\n",
    "\n",
    "        hidden_dim = round(inp * expand_ratio)\n",
    "        self.use_res_connect = self.stride == 1 and inp == oup\n",
    "\n",
    "        if expand_ratio == 1:\n",
    "            # depthwise separable convolution block\n",
    "            self.conv = nn.Sequential(\n",
    "                # dw\n",
    "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.ReLU(inplace=True),\n",
    "                # pw-linear\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "        else:\n",
    "            # Bottleneck with expansion layer\n",
    "            self.conv = nn.Sequential(\n",
    "                # pw\n",
    "                nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.ReLU(inplace=True),\n",
    "                # dw\n",
    "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.ReLU(inplace=True),\n",
    "                # pw-linear\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_res_connect:\n",
    "            # return x + self.conv(x)\n",
    "            return self.add.add(x, self.conv(x))\n",
    "        else:\n",
    "            return self.conv(x)\n",
    "        \n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    14 layers of MobileNetv2 as encoder part\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        block = InvertedResidual\n",
    "        interverted_residual_setting = [\n",
    "            # t, c, n, s\n",
    "            [1, 16, 1, 1],\n",
    "            [6, 24, 2, 2],\n",
    "            [6, 32, 3, 2],\n",
    "            [6, 64, 4, 2],\n",
    "            [6, 96, 3, 1],\n",
    "        ]\n",
    "        # Encoder Part\n",
    "        input_channel = 32 # number of input channels to first inverted (residual) block\n",
    "        self.layers = [conv_bn(3, 32, 2)]\n",
    "        # building inverted residual blocks\n",
    "        for t, c, n, s in interverted_residual_setting:\n",
    "            output_channel = c\n",
    "            for i in range(n):\n",
    "                if i == 0:\n",
    "                    self.layers.append(block(input_channel, output_channel, s, expand_ratio=t))\n",
    "                else:\n",
    "                    self.layers.append(block(input_channel, output_channel, 1, expand_ratio=t))\n",
    "                input_channel = output_channel\n",
    "        # make it nn.Sequential\n",
    "        self.layers = nn.Sequential(*self.layers)\n",
    "                \n",
    "class DecoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder block: upsample and concatenate with features maps from the encoder part\n",
    "    \"\"\"\n",
    "    def __init__(self,up_in_c,x_in_c,upsamplemode='bilinear',expand_ratio=0.15):\n",
    "        super().__init__()\n",
    "        self.cat = nn.quantized.FloatFunctional()\n",
    "\n",
    "        self.upsample = nn.Upsample(scale_factor=2,mode=upsamplemode,align_corners=False) # H, W -> 2H, 2W\n",
    "        self.ir1 = InvertedResidual(up_in_c+x_in_c,(x_in_c + up_in_c) // 2,stride=1,expand_ratio=expand_ratio)\n",
    "\n",
    "    def forward(self,up_in,x_in):\n",
    "        up_out = self.upsample(up_in)\n",
    "        # cat_x = torch.cat([up_out, x_in] , dim=1)\n",
    "        cat_x = self.cat.cat([up_out, x_in] , dim=1)\n",
    "        x = self.ir1(cat_x)\n",
    "        return x\n",
    "    \n",
    "class LastDecoderBlock(nn.Module):\n",
    "    def __init__(self,x_in_c,upsamplemode='bilinear',expand_ratio=0.15, output_channels=1, activation='linear'):\n",
    "        super().__init__()\n",
    "        self.upsample = nn.Upsample(scale_factor=2,mode=upsamplemode,align_corners=False) # H, W -> 2H, 2W\n",
    "        self.ir1 = InvertedResidual(x_in_c,16,stride=1,expand_ratio=expand_ratio)\n",
    "        layers =  [\n",
    "            nn.Conv2d(16, output_channels, 1, 1, 0, bias=True),\n",
    "        ]\n",
    "        self.cat = nn.quantized.FloatFunctional()\n",
    "\n",
    "        if activation == 'sigmoid':\n",
    "            layers.append(nn.Sigmoid())\n",
    "        elif activation == 'softmax':\n",
    "            layers.append(nn.Softmax(dim=1))\n",
    "        elif activation == 'linear' or activation is None:\n",
    "            pass\n",
    "        else:\n",
    "            raise NotImplementedError('Activation {} not implemented'.format(activation))\n",
    "        self.conv = nn.Sequential(\n",
    "           *layers\n",
    "        )\n",
    "\n",
    "    def forward(self,up_in,x_in):\n",
    "        up_out = self.upsample(up_in)\n",
    "        # cat_x = torch.cat([up_out, x_in] , dim=1)\n",
    "        cat_x = self.cat.cat([up_out, x_in] , dim=1)\n",
    "        x = self.ir1(cat_x)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "    \n",
    "class M2UNet(nn.Module):\n",
    "        def __init__(self,encoder,upsamplemode='bilinear',output_channels=1, activation=\"linear\", expand_ratio=0.15):\n",
    "            super(M2UNet,self).__init__()\n",
    "            encoder = list(encoder.children())[0]\n",
    "            # Encoder\n",
    "            self.conv1 = encoder[0:2]\n",
    "            self.conv2 = encoder[2:4]\n",
    "            self.conv3 = encoder[4:7]\n",
    "            self.conv4 = encoder[7:14]\n",
    "            # Decoder\n",
    "            self.decode4 = DecoderBlock(96,32,upsamplemode,expand_ratio)\n",
    "            self.decode3 = DecoderBlock(64,24,upsamplemode,expand_ratio)\n",
    "            self.decode2 = DecoderBlock(44,16,upsamplemode,expand_ratio)\n",
    "            self.decode1 = LastDecoderBlock(33,upsamplemode,expand_ratio, output_channels=output_channels, activation=activation)\n",
    "            # initilaize weights \n",
    "            self._init_params()\n",
    "\n",
    "        def _init_params(self):\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, nn.Conv2d):\n",
    "                    n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                    m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                    if m.bias is not None:\n",
    "                        m.bias.data.zero_()\n",
    "                elif isinstance(m, nn.BatchNorm2d):\n",
    "                    m.weight.data.fill_(1)\n",
    "                    m.bias.data.zero_()\n",
    "                elif isinstance(m, nn.Linear):\n",
    "                    n = m.weight.size(1)\n",
    "                    m.weight.data.normal_(0, 0.01)\n",
    "                    m.bias.data.zero_()\n",
    "            \n",
    "        \n",
    "        \n",
    "        def forward(self,x):\n",
    "            conv1 = self.conv1(x)\n",
    "            conv2 = self.conv2(conv1)\n",
    "            conv3 = self.conv3(conv2)\n",
    "            conv4 = self.conv4(conv3)\n",
    "            decode4 = self.decode4(conv4,conv3)\n",
    "            decode3 = self.decode3(decode4,conv2)\n",
    "            decode2 = self.decode2(decode3,conv1)\n",
    "            decode1 = self.decode1(decode2,x)\n",
    "            return decode1\n",
    "        \n",
    "class M2UNet_q2(nn.Module):\n",
    "        def __init__(self,encoder,upsamplemode='bilinear',output_channels=1, activation=\"linear\", expand_ratio=0.15):\n",
    "            super(M2UNet_q2,self).__init__()\n",
    "            encoder = list(encoder.children())[0]\n",
    "            # Encoder\n",
    "            self.quant = torch.quantization.QuantStub()\n",
    "\n",
    "            self.conv1 = encoder[0:2]\n",
    "            self.conv2 = encoder[2:4]\n",
    "            self.conv3 = encoder[4:7]\n",
    "            self.conv4 = encoder[7:14]\n",
    "            # Decoder\n",
    "            self.decode4 = DecoderBlock(96,32,upsamplemode,expand_ratio)\n",
    "            self.decode3 = DecoderBlock(64,24,upsamplemode,expand_ratio)\n",
    "            self.decode2 = DecoderBlock(44,16,upsamplemode,expand_ratio)\n",
    "            self.decode1 = LastDecoderBlock(33,upsamplemode,expand_ratio, output_channels=output_channels, activation=activation)\n",
    "            \n",
    "            self.dequant = torch.quantization.DeQuantStub()\n",
    "\n",
    "            # initilaize weights \n",
    "            self._init_params()\n",
    "\n",
    "        def _init_params(self):\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, nn.Conv2d):\n",
    "                    n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                    m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                    if m.bias is not None:\n",
    "                        m.bias.data.zero_()\n",
    "                elif isinstance(m, nn.BatchNorm2d):\n",
    "                    m.weight.data.fill_(1)\n",
    "                    m.bias.data.zero_()\n",
    "                elif isinstance(m, nn.Linear):\n",
    "                    n = m.weight.size(1)\n",
    "                    m.weight.data.normal_(0, 0.01)\n",
    "                    m.bias.data.zero_()\n",
    "            \n",
    "        \n",
    "        \n",
    "        def forward(self,x):\n",
    "            x = self.quant(x)\n",
    "\n",
    "            conv1 = self.conv1(x)\n",
    "            conv2 = self.conv2(conv1)\n",
    "            conv3 = self.conv3(conv2)\n",
    "            conv4 = self.conv4(conv3)\n",
    "            decode4 = self.decode4(conv4,conv3)\n",
    "            decode3 = self.decode3(decode4,conv2)\n",
    "            decode2 = self.decode2(decode3,conv1)\n",
    "            decode1 = self.decode1(decode2,x)\n",
    "            \n",
    "            res = self.dequant(decode1)\n",
    "\n",
    "            \n",
    "            return res\n",
    "        \n",
    "def m2unet(output_channels=1,expand_ratio=0.15, activation=\"linear\", **kwargs):\n",
    "    encoder = Encoder()\n",
    "    model = M2UNet(encoder,upsamplemode='bilinear',expand_ratio=expand_ratio, output_channels=output_channels, activation=activation)\n",
    "    return model\n",
    "\n",
    "def m2unet_q2(output_channels=1,expand_ratio=0.15, activation=\"linear\", **kwargs):\n",
    "    encoder = Encoder()\n",
    "    model = M2UNet_q2(encoder,upsamplemode='bilinear',expand_ratio=expand_ratio, output_channels=output_channels, activation=activation)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "09f62fd9-966e-41c3-8b5c-4e4929b94df9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M2UNet_q2(\n",
       "  (quant): QuantStub()\n",
       "  (conv1): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): InvertedResidual(\n",
       "      (add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (conv2): Sequential(\n",
       "    (2): InvertedResidual(\n",
       "      (add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "        (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "        (6): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (3): InvertedResidual(\n",
       "      (add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "        (4): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "        (6): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (conv3): Sequential(\n",
       "    (4): InvertedResidual(\n",
       "      (add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
       "        (4): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "        (6): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): InvertedResidual(\n",
       "      (add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "        (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "        (6): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (6): InvertedResidual(\n",
       "      (add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "        (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "        (6): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (conv4): Sequential(\n",
       "    (7): InvertedResidual(\n",
       "      (add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
       "        (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "        (6): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (8): InvertedResidual(\n",
       "      (add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "        (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "        (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (9): InvertedResidual(\n",
       "      (add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "        (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "        (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (10): InvertedResidual(\n",
       "      (add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "        (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "        (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (11): InvertedResidual(\n",
       "      (add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "        (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "        (6): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (12): InvertedResidual(\n",
       "      (add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "        (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "        (6): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (13): InvertedResidual(\n",
       "      (add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "        (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "        (6): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decode4): DecoderBlock(\n",
       "    (cat): FloatFunctional(\n",
       "      (activation_post_process): Identity()\n",
       "    )\n",
       "    (upsample): Upsample(scale_factor=2.0, mode=bilinear)\n",
       "    (ir1): InvertedResidual(\n",
       "      (add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(128, 19, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(19, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(19, 19, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=19, bias=False)\n",
       "        (4): BatchNorm2d(19, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "        (6): Conv2d(19, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decode3): DecoderBlock(\n",
       "    (cat): FloatFunctional(\n",
       "      (activation_post_process): Identity()\n",
       "    )\n",
       "    (upsample): Upsample(scale_factor=2.0, mode=bilinear)\n",
       "    (ir1): InvertedResidual(\n",
       "      (add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(88, 13, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(13, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=13, bias=False)\n",
       "        (4): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "        (6): Conv2d(13, 44, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (7): BatchNorm2d(44, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decode2): DecoderBlock(\n",
       "    (cat): FloatFunctional(\n",
       "      (activation_post_process): Identity()\n",
       "    )\n",
       "    (upsample): Upsample(scale_factor=2.0, mode=bilinear)\n",
       "    (ir1): InvertedResidual(\n",
       "      (add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(60, 9, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=9, bias=False)\n",
       "        (4): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "        (6): Conv2d(9, 30, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (7): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decode1): LastDecoderBlock(\n",
       "    (upsample): Upsample(scale_factor=2.0, mode=bilinear)\n",
       "    (ir1): InvertedResidual(\n",
       "      (add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(33, 5, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=5, bias=False)\n",
       "        (4): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "        (6): Conv2d(5, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (cat): FloatFunctional(\n",
       "      (activation_post_process): Identity()\n",
       "    )\n",
       "    (conv): Sequential(\n",
       "      (0): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model \n",
    "model = m2unet_q2()\n",
    "model_name = 'm2unet_50epochs_v2_pre_quant'\n",
    "PATH = './models/' + model_name\n",
    "\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b174531d-46e1-43e3-b33a-5a91f14d90b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=32\n",
    "\n",
    "dummy_input=torch.randn(BATCH_SIZE, 3, 512, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14ed5471-ff4f-4a67-a534-5bd961f55499",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n"
     ]
    }
   ],
   "source": [
    "torch.onnx.export(model, dummy_input, \"m2unet_pre_quant_pytorch.onnx\", verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb79cce5-d538-4bec-af63-29c655bfefea",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(resnet50, dummy_input, \"resnet50_pytorch.onnx\", verbose=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b438f018-460b-497f-bb7c-6c89c0fe2b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify we can run the model with dummy input\n",
    "BATCH_SIZE = 1\n",
    "dummy_input=torch.randn(BATCH_SIZE, 3, 1024, 1024)\n",
    "\n",
    "model = m2unet_q2()\n",
    "model_cuda = model.to(\"cuda\").eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b1f46078-0ca1-4a1d-b505-8ead0814a501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write dummy input to torch cuda\n",
    "test_input = dummy_input.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4c5702b2-2348-4556-9e99-cb49c4646fdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 1024, 1024)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    predictions = np.array(model_cuda(test_input).cpu())\n",
    "\n",
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ef4bee34-950d-44ba-8693-a45fbfbed273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.1 ms ± 277 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions = np.array(model_cuda(test_input).cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d8c8f322-58b9-49ca-ad6f-db9432f9df5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model_cuda_half = model_cuda.half()\n",
    "input_half = test_input.half()\n",
    "\n",
    "with torch.no_grad():\n",
    "    preds = np.array(model_cuda_half(input_half).cpu()) # Warm Up\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c191280c-461e-4466-abae-da3de92359f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.6 ms ± 69 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "with torch.no_grad():\n",
    "    preds = np.array(model_cuda_half(input_half).cpu())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcedb92-e58e-408b-8b24-352b8a402d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os._exit(0) # Shut down all kernels so TRT doesn't fight with PyTorch for GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68e5ecc5-c8bf-4658-adf3-f7fc5545c5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "USE_FP16 = True\n",
    "target_dtype = np.float16 if USE_FP16 else np.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb0af816-9cf8-439d-bfd3-7e988a484902",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 224, 224, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from skimage import io\n",
    "from skimage.transform import resize\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "url='https://images.dog.ceo/breeds/retriever-golden/n02099601_3004.jpg'\n",
    "img = resize(io.imread(url), (224, 224))\n",
    "input_batch = np.array(np.repeat(np.expand_dims(np.array(img, dtype=np.float32), axis=0), BATCH_SIZE, axis=0), dtype=np.float32)\n",
    "\n",
    "input_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "911ef711-f2a4-4977-b2d2-2a199e1b9df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.transforms import Normalize\n",
    "\n",
    "def preprocess_image(img):\n",
    "    norm = Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    result = norm(torch.from_numpy(img).transpose(0,2).transpose(1,2))\n",
    "    return np.array(result, dtype=np.float16)\n",
    "\n",
    "preprocessed_images = np.array([preprocess_image(image) for image in input_batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96566ae0-a806-43a7-b099-70316285e3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "663b4805-31dc-4a0d-8904-9d9ed786b727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "&&&& RUNNING TensorRT.trtexec [TensorRT v8003] # trtexec --onnx=resnet50_pytorch.onnx --saveEngine=resnet_engine_pytorch.trt --explicitBatch --inputIOFormats=fp16:chw --outputIOFormats=fp16:chw --fp16\n",
      "[12/15/2022-10:05:02] [I] === Model Options ===\n",
      "[12/15/2022-10:05:02] [I] Format: ONNX\n",
      "[12/15/2022-10:05:02] [I] Model: resnet50_pytorch.onnx\n",
      "[12/15/2022-10:05:02] [I] Output:\n",
      "[12/15/2022-10:05:02] [I] === Build Options ===\n",
      "[12/15/2022-10:05:02] [I] Max batch: explicit\n",
      "[12/15/2022-10:05:02] [I] Workspace: 16 MiB\n",
      "[12/15/2022-10:05:02] [I] minTiming: 1\n",
      "[12/15/2022-10:05:02] [I] avgTiming: 8\n",
      "[12/15/2022-10:05:02] [I] Precision: FP32+FP16\n",
      "[12/15/2022-10:05:02] [I] Calibration: \n",
      "[12/15/2022-10:05:02] [I] Refit: Disabled\n",
      "[12/15/2022-10:05:02] [I] Sparsity: Disabled\n",
      "[12/15/2022-10:05:02] [I] Safe mode: Disabled\n",
      "[12/15/2022-10:05:02] [I] Restricted mode: Disabled\n",
      "[12/15/2022-10:05:02] [I] Save engine: resnet_engine_pytorch.trt\n",
      "[12/15/2022-10:05:02] [I] Load engine: \n",
      "[12/15/2022-10:05:02] [I] NVTX verbosity: 0\n",
      "[12/15/2022-10:05:02] [I] Tactic sources: Using default tactic sources\n",
      "[12/15/2022-10:05:02] [I] timingCacheMode: local\n",
      "[12/15/2022-10:05:02] [I] timingCacheFile: \n",
      "[12/15/2022-10:05:02] [I] Input(s): fp16:chw\n",
      "[12/15/2022-10:05:02] [I] Output(s): fp16:chw\n",
      "[12/15/2022-10:05:02] [I] Input build shapes: model\n",
      "[12/15/2022-10:05:02] [I] Input calibration shapes: model\n",
      "[12/15/2022-10:05:02] [I] === System Options ===\n",
      "[12/15/2022-10:05:02] [I] Device: 0\n",
      "[12/15/2022-10:05:02] [I] DLACore: \n",
      "[12/15/2022-10:05:02] [I] Plugins:\n",
      "[12/15/2022-10:05:02] [I] === Inference Options ===\n",
      "[12/15/2022-10:05:02] [I] Batch: Explicit\n",
      "[12/15/2022-10:05:02] [I] Input inference shapes: model\n",
      "[12/15/2022-10:05:02] [I] Iterations: 10\n",
      "[12/15/2022-10:05:02] [I] Duration: 3s (+ 200ms warm up)\n",
      "[12/15/2022-10:05:02] [I] Sleep time: 0ms\n",
      "[12/15/2022-10:05:02] [I] Streams: 1\n",
      "[12/15/2022-10:05:02] [I] ExposeDMA: Disabled\n",
      "[12/15/2022-10:05:02] [I] Data transfers: Enabled\n",
      "[12/15/2022-10:05:02] [I] Spin-wait: Disabled\n",
      "[12/15/2022-10:05:02] [I] Multithreading: Disabled\n",
      "[12/15/2022-10:05:02] [I] CUDA Graph: Disabled\n",
      "[12/15/2022-10:05:02] [I] Separate profiling: Disabled\n",
      "[12/15/2022-10:05:02] [I] Time Deserialize: Disabled\n",
      "[12/15/2022-10:05:02] [I] Time Refit: Disabled\n",
      "[12/15/2022-10:05:02] [I] Skip inference: Disabled\n",
      "[12/15/2022-10:05:02] [I] Inputs:\n",
      "[12/15/2022-10:05:02] [I] === Reporting Options ===\n",
      "[12/15/2022-10:05:02] [I] Verbose: Disabled\n",
      "[12/15/2022-10:05:02] [I] Averages: 10 inferences\n",
      "[12/15/2022-10:05:02] [I] Percentile: 99\n",
      "[12/15/2022-10:05:02] [I] Dump refittable layers:Disabled\n",
      "[12/15/2022-10:05:02] [I] Dump output: Disabled\n",
      "[12/15/2022-10:05:02] [I] Profile: Disabled\n",
      "[12/15/2022-10:05:02] [I] Export timing to JSON file: \n",
      "[12/15/2022-10:05:02] [I] Export output to JSON file: \n",
      "[12/15/2022-10:05:02] [I] Export profile to JSON file: \n",
      "[12/15/2022-10:05:02] [I] \n",
      "[12/15/2022-10:05:03] [I] === Device Information ===\n",
      "[12/15/2022-10:05:03] [I] Selected Device: NVIDIA A100-SXM4-40GB\n",
      "[12/15/2022-10:05:03] [I] Compute Capability: 8.0\n",
      "[12/15/2022-10:05:03] [I] SMs: 108\n",
      "[12/15/2022-10:05:03] [I] Compute Clock Rate: 1.41 GHz\n",
      "[12/15/2022-10:05:03] [I] Device Global Memory: 40354 MiB\n",
      "[12/15/2022-10:05:03] [I] Shared Memory per SM: 164 KiB\n",
      "[12/15/2022-10:05:03] [I] Memory Bus Width: 5120 bits (ECC enabled)\n",
      "[12/15/2022-10:05:03] [I] Memory Clock Rate: 1.215 GHz\n",
      "[12/15/2022-10:05:03] [I] \n",
      "[12/15/2022-10:05:03] [I] TensorRT version: 8003\n",
      "[12/15/2022-10:05:06] [I] [TRT] [MemUsageChange] Init CUDA: CPU +501, GPU +0, now: CPU 508, GPU 727 (MiB)\n",
      "[12/15/2022-10:05:06] [I] Start parsing network model\n",
      "[12/15/2022-10:05:06] [I] [TRT] ----------------------------------------------------------------\n",
      "[12/15/2022-10:05:06] [I] [TRT] Input filename:   resnet50_pytorch.onnx\n",
      "[12/15/2022-10:05:06] [I] [TRT] ONNX IR version:  0.0.7\n",
      "[12/15/2022-10:05:06] [I] [TRT] Opset version:    13\n",
      "[12/15/2022-10:05:06] [I] [TRT] Producer name:    pytorch\n",
      "[12/15/2022-10:05:06] [I] [TRT] Producer version: 1.12.1\n",
      "[12/15/2022-10:05:06] [I] [TRT] Domain:           \n",
      "[12/15/2022-10:05:06] [I] [TRT] Model version:    0\n",
      "[12/15/2022-10:05:06] [I] [TRT] Doc string:       \n",
      "[12/15/2022-10:05:06] [I] [TRT] ----------------------------------------------------------------\n",
      "[12/15/2022-10:05:06] [I] Finish parsing network model\n",
      "[12/15/2022-10:05:06] [I] [TRT] [MemUsageChange] Init CUDA: CPU +0, GPU +0, now: CPU 604, GPU 727 (MiB)\n",
      "[12/15/2022-10:05:06] [I] [TRT] [MemUsageSnapshot] Builder begin: CPU 604 MiB, GPU 727 MiB\n",
      "[12/15/2022-10:05:08] [W] [TRT] TensorRT was linked against cuBLAS/cuBLAS LT 11.5.1 but loaded cuBLAS/cuBLAS LT 11.2.0\n",
      "[12/15/2022-10:05:08] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +625, GPU +262, now: CPU 1229, GPU 989 (MiB)\n",
      "[12/15/2022-10:05:08] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +414, GPU +256, now: CPU 1643, GPU 1245 (MiB)\n",
      "[12/15/2022-10:05:08] [W] [TRT] TensorRT was linked against cuDNN 8.2.1 but loaded cuDNN 8.0.5\n",
      "[12/15/2022-10:05:08] [W] [TRT] Detected invalid timing cache, setup a local cache instead\n",
      "[12/15/2022-10:05:10] [I] [TRT] Some tactics do not have sufficient workspace memory to run. Increasing workspace size may increase performance, please check verbose output.\n",
      "[12/15/2022-10:07:15] [I] [TRT] Detected 1 inputs and 1 output network tensors.\n",
      "[12/15/2022-10:07:15] [I] [TRT] Total Host Persistent Memory: 199152\n",
      "[12/15/2022-10:07:15] [I] [TRT] Total Device Persistent Memory: 51573760\n",
      "[12/15/2022-10:07:15] [I] [TRT] Total Scratch Memory: 0\n",
      "[12/15/2022-10:07:15] [I] [TRT] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 53 MiB, GPU 256 MiB\n",
      "[12/15/2022-10:07:15] [W] [TRT] TensorRT was linked against cuBLAS/cuBLAS LT 11.5.1 but loaded cuBLAS/cuBLAS LT 11.2.0\n",
      "[12/15/2022-10:07:15] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 2192, GPU 1523 (MiB)\n",
      "[12/15/2022-10:07:15] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +8, now: CPU 2192, GPU 1531 (MiB)\n",
      "[12/15/2022-10:07:15] [W] [TRT] TensorRT was linked against cuDNN 8.2.1 but loaded cuDNN 8.0.5\n",
      "[12/15/2022-10:07:15] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +0, now: CPU 2192, GPU 1515 (MiB)\n",
      "[12/15/2022-10:07:15] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +0, now: CPU 2191, GPU 1497 (MiB)\n",
      "[12/15/2022-10:07:15] [I] [TRT] [MemUsageSnapshot] Builder end: CPU 2191 MiB, GPU 1497 MiB\n",
      "[12/15/2022-10:07:15] [I] [TRT] Loaded engine size: 49 MB\n",
      "[12/15/2022-10:07:15] [I] [TRT] [MemUsageSnapshot] deserializeCudaEngine begin: CPU 2187 MiB, GPU 1447 MiB\n",
      "[12/15/2022-10:07:15] [W] [TRT] TensorRT was linked against cuBLAS/cuBLAS LT 11.5.1 but loaded cuBLAS/cuBLAS LT 11.2.0\n",
      "[12/15/2022-10:07:15] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +10, now: CPU 2188, GPU 1507 (MiB)\n",
      "[12/15/2022-10:07:15] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +8, now: CPU 2188, GPU 1515 (MiB)\n",
      "[12/15/2022-10:07:15] [W] [TRT] TensorRT was linked against cuDNN 8.2.1 but loaded cuDNN 8.0.5\n",
      "[12/15/2022-10:07:15] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +0, now: CPU 2188, GPU 1497 (MiB)\n",
      "[12/15/2022-10:07:15] [I] [TRT] [MemUsageSnapshot] deserializeCudaEngine end: CPU 2188 MiB, GPU 1497 MiB\n",
      "[12/15/2022-10:07:15] [I] Engine built in 131.95 sec.\n",
      "[12/15/2022-10:07:15] [I] [TRT] [MemUsageSnapshot] ExecutionContext creation begin: CPU 2039 MiB, GPU 1497 MiB\n",
      "[12/15/2022-10:07:15] [W] [TRT] TensorRT was linked against cuBLAS/cuBLAS LT 11.5.1 but loaded cuBLAS/cuBLAS LT 11.2.0\n",
      "[12/15/2022-10:07:15] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +1, GPU +10, now: CPU 2040, GPU 1507 (MiB)\n",
      "[12/15/2022-10:07:15] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +8, now: CPU 2040, GPU 1515 (MiB)\n",
      "[12/15/2022-10:07:15] [W] [TRT] TensorRT was linked against cuDNN 8.2.1 but loaded cuDNN 8.0.5\n",
      "[12/15/2022-10:07:15] [I] [TRT] [MemUsageSnapshot] ExecutionContext creation end: CPU 2040 MiB, GPU 2141 MiB\n",
      "[12/15/2022-10:07:16] [I] Created input binding for input.1 with dimensions 32x3x512x512\n",
      "[12/15/2022-10:07:16] [I] Created output binding for 495 with dimensions 32x1000\n",
      "[12/15/2022-10:07:16] [I] Starting inference\n",
      "[12/15/2022-10:07:19] [I] Warmup completed 15 queries over 200 ms\n",
      "[12/15/2022-10:07:19] [I] Timing trace has 261 queries over 3.03003 s\n",
      "[12/15/2022-10:07:19] [I] \n",
      "[12/15/2022-10:07:19] [I] === Trace details ===\n",
      "[12/15/2022-10:07:19] [I] Trace averages of 10 runs:\n",
      "[12/15/2022-10:07:19] [I] Average on 10 runs - GPU latency: 11.6381 ms - Host latency: 15.7359 ms (end to end 23.3664 ms, enqueue 0.395898 ms)\n",
      "[12/15/2022-10:07:19] [I] Average on 10 runs - GPU latency: 11.5415 ms - Host latency: 15.6427 ms (end to end 22.9735 ms, enqueue 0.390897 ms)\n",
      "[12/15/2022-10:07:19] [I] Average on 10 runs - GPU latency: 11.5545 ms - Host latency: 15.6535 ms (end to end 23.0072 ms, enqueue 0.380933 ms)\n",
      "[12/15/2022-10:07:19] [I] Average on 10 runs - GPU latency: 11.5551 ms - Host latency: 15.6534 ms (end to end 23.0017 ms, enqueue 0.39184 ms)\n",
      "[12/15/2022-10:07:19] [I] Average on 10 runs - GPU latency: 11.54 ms - Host latency: 15.6377 ms (end to end 22.9791 ms, enqueue 0.377124 ms)\n",
      "[12/15/2022-10:07:19] [I] Average on 10 runs - GPU latency: 11.5363 ms - Host latency: 15.6355 ms (end to end 22.9573 ms, enqueue 0.428339 ms)\n",
      "[12/15/2022-10:07:19] [I] Average on 10 runs - GPU latency: 11.552 ms - Host latency: 15.6513 ms (end to end 22.9818 ms, enqueue 0.446637 ms)\n",
      "[12/15/2022-10:07:19] [I] Average on 10 runs - GPU latency: 11.5531 ms - Host latency: 15.6516 ms (end to end 23.0074 ms, enqueue 0.370905 ms)\n",
      "[12/15/2022-10:07:19] [I] Average on 10 runs - GPU latency: 11.5432 ms - Host latency: 15.6419 ms (end to end 22.9846 ms, enqueue 0.376843 ms)\n",
      "[12/15/2022-10:07:19] [I] Average on 10 runs - GPU latency: 11.543 ms - Host latency: 15.6411 ms (end to end 22.9849 ms, enqueue 0.373975 ms)\n",
      "[12/15/2022-10:07:19] [I] Average on 10 runs - GPU latency: 11.5516 ms - Host latency: 15.6496 ms (end to end 23.012 ms, enqueue 0.371582 ms)\n",
      "[12/15/2022-10:07:19] [I] Average on 10 runs - GPU latency: 11.5514 ms - Host latency: 15.6493 ms (end to end 23.0037 ms, enqueue 0.373523 ms)\n",
      "[12/15/2022-10:07:19] [I] Average on 10 runs - GPU latency: 11.5519 ms - Host latency: 15.6498 ms (end to end 23.0085 ms, enqueue 0.373645 ms)\n",
      "[12/15/2022-10:07:19] [I] Average on 10 runs - GPU latency: 11.5504 ms - Host latency: 15.6487 ms (end to end 23.0067 ms, enqueue 0.376953 ms)\n",
      "[12/15/2022-10:07:19] [I] Average on 10 runs - GPU latency: 11.5635 ms - Host latency: 15.6621 ms (end to end 23.0296 ms, enqueue 0.377466 ms)\n",
      "[12/15/2022-10:07:19] [I] Average on 10 runs - GPU latency: 11.5532 ms - Host latency: 15.6515 ms (end to end 23.0128 ms, enqueue 0.378479 ms)\n",
      "[12/15/2022-10:07:19] [I] Average on 10 runs - GPU latency: 11.5552 ms - Host latency: 15.6533 ms (end to end 23.0169 ms, enqueue 0.381067 ms)\n",
      "[12/15/2022-10:07:19] [I] Average on 10 runs - GPU latency: 11.5529 ms - Host latency: 15.6508 ms (end to end 23.0152 ms, enqueue 0.376221 ms)\n",
      "[12/15/2022-10:07:19] [I] Average on 10 runs - GPU latency: 11.5525 ms - Host latency: 15.6509 ms (end to end 22.982 ms, enqueue 0.388037 ms)\n",
      "[12/15/2022-10:07:19] [I] Average on 10 runs - GPU latency: 11.5361 ms - Host latency: 15.6343 ms (end to end 22.9856 ms, enqueue 0.382397 ms)\n",
      "[12/15/2022-10:07:19] [I] Average on 10 runs - GPU latency: 11.5504 ms - Host latency: 15.6484 ms (end to end 22.9861 ms, enqueue 0.372876 ms)\n",
      "[12/15/2022-10:07:19] [I] Average on 10 runs - GPU latency: 11.5558 ms - Host latency: 15.6544 ms (end to end 23.0167 ms, enqueue 0.38125 ms)\n",
      "[12/15/2022-10:07:19] [I] Average on 10 runs - GPU latency: 11.5495 ms - Host latency: 15.649 ms (end to end 22.9916 ms, enqueue 0.449536 ms)\n",
      "[12/15/2022-10:07:19] [I] Average on 10 runs - GPU latency: 11.5553 ms - Host latency: 15.6541 ms (end to end 23.0065 ms, enqueue 0.416309 ms)\n",
      "[12/15/2022-10:07:19] [I] Average on 10 runs - GPU latency: 11.5543 ms - Host latency: 15.6523 ms (end to end 23.0156 ms, enqueue 0.379639 ms)\n",
      "[12/15/2022-10:07:19] [I] Average on 10 runs - GPU latency: 11.5532 ms - Host latency: 15.6513 ms (end to end 23.0053 ms, enqueue 0.378784 ms)\n",
      "[12/15/2022-10:07:19] [I] \n",
      "[12/15/2022-10:07:19] [I] === Performance summary ===\n",
      "[12/15/2022-10:07:19] [I] Throughput: 86.1378 qps\n",
      "[12/15/2022-10:07:19] [I] Latency: min = 15.6226 ms, max = 16.5497 ms, mean = 15.652 ms, median = 15.6501 ms, percentile(99%) = 15.6785 ms\n",
      "[12/15/2022-10:07:19] [I] End-to-End Host Latency: min = 22.7805 ms, max = 25.8353 ms, mean = 23.0129 ms, median = 23.0034 ms, percentile(99%) = 23.0885 ms\n",
      "[12/15/2022-10:07:19] [I] Enqueue Time: min = 0.355957 ms, max = 0.516235 ms, mean = 0.388072 ms, median = 0.377441 ms, percentile(99%) = 0.48999 ms\n",
      "[12/15/2022-10:07:19] [I] H2D Latency: min = 4.08398 ms, max = 4.11673 ms, mean = 4.08623 ms, median = 4.08582 ms, percentile(99%) = 4.09143 ms\n",
      "[12/15/2022-10:07:19] [I] GPU Compute Time: min = 11.5251 ms, max = 12.4518 ms, mean = 11.5535 ms, median = 11.5516 ms, percentile(99%) = 11.5804 ms\n",
      "[12/15/2022-10:07:19] [I] D2H Latency: min = 0.010498 ms, max = 0.0144043 ms, mean = 0.0122422 ms, median = 0.012146 ms, percentile(99%) = 0.013916 ms\n",
      "[12/15/2022-10:07:19] [I] Total Host Walltime: 3.03003 s\n",
      "[12/15/2022-10:07:19] [I] Total GPU Compute Time: 3.01547 s\n",
      "[12/15/2022-10:07:19] [I] Explanations of the performance metrics are printed in the verbose logs.\n",
      "[12/15/2022-10:07:19] [I] \n",
      "&&&& PASSED TensorRT.trtexec [TensorRT v8003] # trtexec --onnx=resnet50_pytorch.onnx --saveEngine=resnet_engine_pytorch.trt --explicitBatch --inputIOFormats=fp16:chw --outputIOFormats=fp16:chw --fp16\n",
      "[12/15/2022-10:07:19] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +0, now: CPU 2040, GPU 2073 (MiB)\n"
     ]
    }
   ],
   "source": [
    "# # step out of Python for a moment to convert the ONNX model to a TRT engine using trtexec\n",
    "# if USE_FP16:\n",
    "#     !trtexec --onnx=resnet50_pytorch.onnx --saveEngine=resnet_engine_pytorch.trt  --explicitBatch --inputIOFormats=fp16:chw --outputIOFormats=fp16:chw --fp16\n",
    "# else:\n",
    "#     !trtexec --onnx=resnet50_pytorch.onnx --saveEngine=resnet_engine_pytorch.trt  --explicitBatch\n",
    "\n",
    "\n",
    "# step out of Python for a moment to convert the ONNX model to a TRT engine using trtexec\n",
    "if USE_FP16:\n",
    "    !trtexec --onnx=resnet50_pytorch.onnx --saveEngine=resnet_engine_pytorch.trt  --explicitBatch --inputIOFormats=fp16:chw --outputIOFormats=fp16:chw --fp16\n",
    "else:\n",
    "    !trtexec --onnx=resnet50_pytorch.onnx --saveEngine=resnet_engine_pytorch.trt  --explicitBatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db7b1207-92b4-4cb0-a43a-05d548485934",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M2UNet_q2(\n",
       "  (quant): QuantStub()\n",
       "  (conv1): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): InvertedResidual(\n",
       "      (add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (conv2): Sequential(\n",
       "    (2): InvertedResidual(\n",
       "      (add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "        (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "        (6): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (3): InvertedResidual(\n",
       "      (add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "        (4): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "        (6): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (conv3): Sequential(\n",
       "    (4): InvertedResidual(\n",
       "      (add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
       "        (4): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "        (6): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): InvertedResidual(\n",
       "      (add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "        (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "        (6): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (6): InvertedResidual(\n",
       "      (add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "        (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "        (6): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (conv4): Sequential(\n",
       "    (7): InvertedResidual(\n",
       "      (add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
       "        (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "        (6): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (8): InvertedResidual(\n",
       "      (add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "        (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "        (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (9): InvertedResidual(\n",
       "      (add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "        (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "        (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (10): InvertedResidual(\n",
       "      (add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "        (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "        (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (11): InvertedResidual(\n",
       "      (add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "        (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "        (6): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (12): InvertedResidual(\n",
       "      (add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "        (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "        (6): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (13): InvertedResidual(\n",
       "      (add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "        (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "        (6): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decode4): DecoderBlock(\n",
       "    (cat): FloatFunctional(\n",
       "      (activation_post_process): Identity()\n",
       "    )\n",
       "    (upsample): Upsample(scale_factor=2.0, mode=bilinear)\n",
       "    (ir1): InvertedResidual(\n",
       "      (add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(128, 19, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(19, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(19, 19, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=19, bias=False)\n",
       "        (4): BatchNorm2d(19, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "        (6): Conv2d(19, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decode3): DecoderBlock(\n",
       "    (cat): FloatFunctional(\n",
       "      (activation_post_process): Identity()\n",
       "    )\n",
       "    (upsample): Upsample(scale_factor=2.0, mode=bilinear)\n",
       "    (ir1): InvertedResidual(\n",
       "      (add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(88, 13, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(13, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=13, bias=False)\n",
       "        (4): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "        (6): Conv2d(13, 44, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (7): BatchNorm2d(44, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decode2): DecoderBlock(\n",
       "    (cat): FloatFunctional(\n",
       "      (activation_post_process): Identity()\n",
       "    )\n",
       "    (upsample): Upsample(scale_factor=2.0, mode=bilinear)\n",
       "    (ir1): InvertedResidual(\n",
       "      (add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(60, 9, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=9, bias=False)\n",
       "        (4): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "        (6): Conv2d(9, 30, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (7): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decode1): LastDecoderBlock(\n",
       "    (upsample): Upsample(scale_factor=2.0, mode=bilinear)\n",
       "    (ir1): InvertedResidual(\n",
       "      (add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(33, 5, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=5, bias=False)\n",
       "        (4): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "        (6): Conv2d(5, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (cat): FloatFunctional(\n",
       "      (activation_post_process): Identity()\n",
       "    )\n",
       "    (conv): Sequential(\n",
       "      (0): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model and convert to onnx\n",
    "\n",
    "# Load model \n",
    "model = m2unet_q2()\n",
    "model_name = 'm2unet_pre_quant_12_22'\n",
    "PATH = './models/' + model_name\n",
    "\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "022f121f-498a-43b3-9726-0966694768ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=32\n",
    "\n",
    "dummy_input=torch.randn(BATCH_SIZE, 3, 224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "42d58b5e-d475-41ea-be73-8d261a20c0d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported graph: graph(%input.1 : Float(32, 3, 224, 224, strides=[150528, 50176, 224, 1], requires_grad=0, device=cpu),\n",
      "      %decode1.conv.0.weight : Float(1, 16, 1, 1, strides=[16, 1, 1, 1], requires_grad=1, device=cpu),\n",
      "      %decode1.conv.0.bias : Float(1, strides=[1], requires_grad=1, device=cpu),\n",
      "      %onnx::Conv_479 : Float(32, 3, 3, 3, strides=[27, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_480 : Float(32, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_482 : Float(32, 1, 3, 3, strides=[9, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_483 : Float(32, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_485 : Float(16, 32, 1, 1, strides=[32, 1, 1, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_486 : Float(16, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_488 : Float(96, 16, 1, 1, strides=[16, 1, 1, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_489 : Float(96, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_491 : Float(96, 1, 3, 3, strides=[9, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_492 : Float(96, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_494 : Float(24, 96, 1, 1, strides=[96, 1, 1, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_495 : Float(24, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_497 : Float(144, 24, 1, 1, strides=[24, 1, 1, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_498 : Float(144, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_500 : Float(144, 1, 3, 3, strides=[9, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_501 : Float(144, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_503 : Float(24, 144, 1, 1, strides=[144, 1, 1, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_504 : Float(24, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_506 : Float(144, 24, 1, 1, strides=[24, 1, 1, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_507 : Float(144, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_509 : Float(144, 1, 3, 3, strides=[9, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_510 : Float(144, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_512 : Float(32, 144, 1, 1, strides=[144, 1, 1, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_513 : Float(32, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_515 : Float(192, 32, 1, 1, strides=[32, 1, 1, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_516 : Float(192, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_518 : Float(192, 1, 3, 3, strides=[9, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_519 : Float(192, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_521 : Float(32, 192, 1, 1, strides=[192, 1, 1, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_522 : Float(32, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_524 : Float(192, 32, 1, 1, strides=[32, 1, 1, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_525 : Float(192, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_527 : Float(192, 1, 3, 3, strides=[9, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_528 : Float(192, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_530 : Float(32, 192, 1, 1, strides=[192, 1, 1, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_531 : Float(32, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_533 : Float(192, 32, 1, 1, strides=[32, 1, 1, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_534 : Float(192, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_536 : Float(192, 1, 3, 3, strides=[9, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_537 : Float(192, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_539 : Float(64, 192, 1, 1, strides=[192, 1, 1, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_540 : Float(64, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_542 : Float(384, 64, 1, 1, strides=[64, 1, 1, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_543 : Float(384, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_545 : Float(384, 1, 3, 3, strides=[9, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_546 : Float(384, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_548 : Float(64, 384, 1, 1, strides=[384, 1, 1, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_549 : Float(64, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_551 : Float(384, 64, 1, 1, strides=[64, 1, 1, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_552 : Float(384, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_554 : Float(384, 1, 3, 3, strides=[9, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_555 : Float(384, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_557 : Float(64, 384, 1, 1, strides=[384, 1, 1, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_558 : Float(64, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_560 : Float(384, 64, 1, 1, strides=[64, 1, 1, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_561 : Float(384, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_563 : Float(384, 1, 3, 3, strides=[9, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_564 : Float(384, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_566 : Float(64, 384, 1, 1, strides=[384, 1, 1, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_567 : Float(64, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_569 : Float(384, 64, 1, 1, strides=[64, 1, 1, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_570 : Float(384, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_572 : Float(384, 1, 3, 3, strides=[9, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_573 : Float(384, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_575 : Float(96, 384, 1, 1, strides=[384, 1, 1, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_576 : Float(96, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_578 : Float(576, 96, 1, 1, strides=[96, 1, 1, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_579 : Float(576, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_581 : Float(576, 1, 3, 3, strides=[9, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_582 : Float(576, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_584 : Float(96, 576, 1, 1, strides=[576, 1, 1, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_585 : Float(96, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_587 : Float(576, 96, 1, 1, strides=[96, 1, 1, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_588 : Float(576, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_590 : Float(576, 1, 3, 3, strides=[9, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_591 : Float(576, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_593 : Float(96, 576, 1, 1, strides=[576, 1, 1, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_594 : Float(96, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_596 : Float(19, 128, 1, 1, strides=[128, 1, 1, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_597 : Float(19, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_599 : Float(19, 1, 3, 3, strides=[9, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_600 : Float(19, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_602 : Float(64, 19, 1, 1, strides=[19, 1, 1, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_603 : Float(64, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_605 : Float(13, 88, 1, 1, strides=[88, 1, 1, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_606 : Float(13, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_608 : Float(13, 1, 3, 3, strides=[9, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_609 : Float(13, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_611 : Float(44, 13, 1, 1, strides=[13, 1, 1, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_612 : Float(44, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_614 : Float(9, 60, 1, 1, strides=[60, 1, 1, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_615 : Float(9, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_617 : Float(9, 1, 3, 3, strides=[9, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_618 : Float(9, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_620 : Float(30, 9, 1, 1, strides=[9, 1, 1, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_621 : Float(30, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_623 : Float(5, 33, 1, 1, strides=[33, 1, 1, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_624 : Float(5, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_626 : Float(5, 1, 3, 3, strides=[9, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_627 : Float(5, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_629 : Float(16, 5, 1, 1, strides=[5, 1, 1, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_630 : Float(16, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Resize_631 : Float(4, strides=[1], requires_grad=0, device=cpu)):\n",
      "  %onnx::Resize_634 : Float(4, strides=[1], requires_grad=0, device=cpu) = onnx::Identity[onnx_name=\"Identity_0\"](%onnx::Resize_631)\n",
      "  %onnx::Resize_633 : Float(4, strides=[1], requires_grad=0, device=cpu) = onnx::Identity[onnx_name=\"Identity_1\"](%onnx::Resize_631)\n",
      "  %onnx::Resize_632 : Float(4, strides=[1], requires_grad=0, device=cpu) = onnx::Identity[onnx_name=\"Identity_2\"](%onnx::Resize_631)\n",
      "  %input.4 : Float(32, 32, 112, 112, strides=[401408, 12544, 112, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2], onnx_name=\"Conv_3\"](%input.1, %onnx::Conv_479, %onnx::Conv_480) # /opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py:454:0\n",
      "  %onnx::Conv_311 : Float(32, 32, 112, 112, strides=[401408, 12544, 112, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"Relu_4\"](%input.4) # /opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1455:0\n",
      "  %input.12 : Float(32, 32, 112, 112, strides=[401408, 12544, 112, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=32, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"Conv_5\"](%onnx::Conv_311, %onnx::Conv_482, %onnx::Conv_483) # /opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py:454:0\n",
      "  %onnx::Conv_314 : Float(32, 32, 112, 112, strides=[401408, 12544, 112, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"Relu_6\"](%input.12) # /opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1455:0\n",
      "  %input.20 : Float(32, 16, 112, 112, strides=[200704, 12544, 112, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1], onnx_name=\"Conv_7\"](%onnx::Conv_314, %onnx::Conv_485, %onnx::Conv_486) # /opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py:454:0\n",
      "  %input.28 : Float(32, 96, 112, 112, strides=[1204224, 12544, 112, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1], onnx_name=\"Conv_8\"](%input.20, %onnx::Conv_488, %onnx::Conv_489) # /opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py:454:0\n",
      "  %onnx::Conv_319 : Float(32, 96, 112, 112, strides=[1204224, 12544, 112, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"Relu_9\"](%input.28) # /opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1455:0\n",
      "  %input.36 : Float(32, 96, 56, 56, strides=[301056, 3136, 56, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=96, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2], onnx_name=\"Conv_10\"](%onnx::Conv_319, %onnx::Conv_491, %onnx::Conv_492) # /opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py:454:0\n",
      "  %onnx::Conv_322 : Float(32, 96, 56, 56, strides=[301056, 3136, 56, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"Relu_11\"](%input.36) # /opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1455:0\n",
      "  %input.44 : Float(32, 24, 56, 56, strides=[75264, 3136, 56, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1], onnx_name=\"Conv_12\"](%onnx::Conv_322, %onnx::Conv_494, %onnx::Conv_495) # /opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py:454:0\n",
      "  %input.52 : Float(32, 144, 56, 56, strides=[451584, 3136, 56, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1], onnx_name=\"Conv_13\"](%input.44, %onnx::Conv_497, %onnx::Conv_498) # /opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py:454:0\n",
      "  %onnx::Conv_327 : Float(32, 144, 56, 56, strides=[451584, 3136, 56, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"Relu_14\"](%input.52) # /opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1455:0\n",
      "  %input.60 : Float(32, 144, 56, 56, strides=[451584, 3136, 56, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=144, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"Conv_15\"](%onnx::Conv_327, %onnx::Conv_500, %onnx::Conv_501) # /opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py:454:0\n",
      "  %onnx::Conv_330 : Float(32, 144, 56, 56, strides=[451584, 3136, 56, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"Relu_16\"](%input.60) # /opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1455:0\n",
      "  %onnx::Add_502 : Float(32, 24, 56, 56, strides=[75264, 3136, 56, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1], onnx_name=\"Conv_17\"](%onnx::Conv_330, %onnx::Conv_503, %onnx::Conv_504) # /opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py:454:0\n",
      "  %input.68 : Float(32, 24, 56, 56, strides=[75264, 3136, 56, 1], requires_grad=1, device=cpu) = onnx::Add[onnx_name=\"Add_18\"](%input.44, %onnx::Add_502) # /opt/conda/lib/python3.7/site-packages/torch/nn/quantized/modules/functional_modules.py:43:0\n",
      "  %input.76 : Float(32, 144, 56, 56, strides=[451584, 3136, 56, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1], onnx_name=\"Conv_19\"](%input.68, %onnx::Conv_506, %onnx::Conv_507) # /opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py:454:0\n",
      "  %onnx::Conv_336 : Float(32, 144, 56, 56, strides=[451584, 3136, 56, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"Relu_20\"](%input.76) # /opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1455:0\n",
      "  %input.84 : Float(32, 144, 28, 28, strides=[112896, 784, 28, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=144, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2], onnx_name=\"Conv_21\"](%onnx::Conv_336, %onnx::Conv_509, %onnx::Conv_510) # /opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py:454:0\n",
      "  %onnx::Conv_339 : Float(32, 144, 28, 28, strides=[112896, 784, 28, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"Relu_22\"](%input.84) # /opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1455:0\n",
      "  %input.92 : Float(32, 32, 28, 28, strides=[25088, 784, 28, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1], onnx_name=\"Conv_23\"](%onnx::Conv_339, %onnx::Conv_512, %onnx::Conv_513) # /opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py:454:0\n",
      "  %input.100 : Float(32, 192, 28, 28, strides=[150528, 784, 28, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1], onnx_name=\"Conv_24\"](%input.92, %onnx::Conv_515, %onnx::Conv_516) # /opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py:454:0\n",
      "  %onnx::Conv_344 : Float(32, 192, 28, 28, strides=[150528, 784, 28, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"Relu_25\"](%input.100) # /opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1455:0\n",
      "  %input.108 : Float(32, 192, 28, 28, strides=[150528, 784, 28, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=192, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"Conv_26\"](%onnx::Conv_344, %onnx::Conv_518, %onnx::Conv_519) # /opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py:454:0\n",
      "  %onnx::Conv_347 : Float(32, 192, 28, 28, strides=[150528, 784, 28, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"Relu_27\"](%input.108) # /opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1455:0\n",
      "  %onnx::Add_520 : Float(32, 32, 28, 28, strides=[25088, 784, 28, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1], onnx_name=\"Conv_28\"](%onnx::Conv_347, %onnx::Conv_521, %onnx::Conv_522) # /opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py:454:0\n",
      "  %input.116 : Float(32, 32, 28, 28, strides=[25088, 784, 28, 1], requires_grad=1, device=cpu) = onnx::Add[onnx_name=\"Add_29\"](%input.92, %onnx::Add_520) # /opt/conda/lib/python3.7/site-packages/torch/nn/quantized/modules/functional_modules.py:43:0\n",
      "  %input.124 : Float(32, 192, 28, 28, strides=[150528, 784, 28, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1], onnx_name=\"Conv_30\"](%input.116, %onnx::Conv_524, %onnx::Conv_525) # /opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py:454:0\n",
      "  %onnx::Conv_353 : Float(32, 192, 28, 28, strides=[150528, 784, 28, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"Relu_31\"](%input.124) # /opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1455:0\n",
      "  %input.132 : Float(32, 192, 28, 28, strides=[150528, 784, 28, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=192, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"Conv_32\"](%onnx::Conv_353, %onnx::Conv_527, %onnx::Conv_528) # /opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py:454:0\n",
      "  %onnx::Conv_356 : Float(32, 192, 28, 28, strides=[150528, 784, 28, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"Relu_33\"](%input.132) # /opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1455:0\n",
      "  %onnx::Add_529 : Float(32, 32, 28, 28, strides=[25088, 784, 28, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1], onnx_name=\"Conv_34\"](%onnx::Conv_356, %onnx::Conv_530, %onnx::Conv_531) # /opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py:454:0\n",
      "  %input.140 : Float(32, 32, 28, 28, strides=[25088, 784, 28, 1], requires_grad=1, device=cpu) = onnx::Add[onnx_name=\"Add_35\"](%input.116, %onnx::Add_529) # /opt/conda/lib/python3.7/site-packages/torch/nn/quantized/modules/functional_modules.py:43:0\n",
      "  %input.148 : Float(32, 192, 28, 28, strides=[150528, 784, 28, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1], onnx_name=\"Conv_36\"](%input.140, %onnx::Conv_533, %onnx::Conv_534) # /opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py:454:0\n",
      "  %onnx::Conv_362 : Float(32, 192, 28, 28, strides=[150528, 784, 28, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"Relu_37\"](%input.148) # /opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1455:0\n",
      "  %input.156 : Float(32, 192, 14, 14, strides=[37632, 196, 14, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=192, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2], onnx_name=\"Conv_38\"](%onnx::Conv_362, %onnx::Conv_536, %onnx::Conv_537) # /opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py:454:0\n",
      "  %onnx::Conv_365 : Float(32, 192, 14, 14, strides=[37632, 196, 14, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"Relu_39\"](%input.156) # /opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1455:0\n",
      "  %input.164 : Float(32, 64, 14, 14, strides=[12544, 196, 14, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1], onnx_name=\"Conv_40\"](%onnx::Conv_365, %onnx::Conv_539, %onnx::Conv_540) # /opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py:454:0\n",
      "  %input.172 : Float(32, 384, 14, 14, strides=[75264, 196, 14, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1], onnx_name=\"Conv_41\"](%input.164, %onnx::Conv_542, %onnx::Conv_543) # /opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py:454:0\n",
      "  %onnx::Conv_370 : Float(32, 384, 14, 14, strides=[75264, 196, 14, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"Relu_42\"](%input.172) # /opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1455:0\n",
      "  %input.180 : Float(32, 384, 14, 14, strides=[75264, 196, 14, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=384, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"Conv_43\"](%onnx::Conv_370, %onnx::Conv_545, %onnx::Conv_546) # /opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py:454:0\n",
      "  %onnx::Conv_373 : Float(32, 384, 14, 14, strides=[75264, 196, 14, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"Relu_44\"](%input.180) # /opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1455:0\n",
      "  %onnx::Add_547 : Float(32, 64, 14, 14, strides=[12544, 196, 14, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1], onnx_name=\"Conv_45\"](%onnx::Conv_373, %onnx::Conv_548, %onnx::Conv_549) # /opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py:454:0\n",
      "  %input.188 : Float(32, 64, 14, 14, strides=[12544, 196, 14, 1], requires_grad=1, device=cpu) = onnx::Add[onnx_name=\"Add_46\"](%input.164, %onnx::Add_547) # /opt/conda/lib/python3.7/site-packages/torch/nn/quantized/modules/functional_modules.py:43:0\n",
      "  %input.196 : Float(32, 384, 14, 14, strides=[75264, 196, 14, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1], onnx_name=\"Conv_47\"](%input.188, %onnx::Conv_551, %onnx::Conv_552) # /opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py:454:0\n",
      "  %onnx::Conv_379 : Float(32, 384, 14, 14, strides=[75264, 196, 14, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"Relu_48\"](%input.196) # /opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1455:0\n",
      "  %input.204 : Float(32, 384, 14, 14, strides=[75264, 196, 14, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=384, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"Conv_49\"](%onnx::Conv_379, %onnx::Conv_554, %onnx::Conv_555) # /opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py:454:0\n",
      "  %onnx::Conv_382 : Float(32, 384, 14, 14, strides=[75264, 196, 14, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"Relu_50\"](%input.204) # /opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1455:0\n",
      "  %onnx::Add_556 : Float(32, 64, 14, 14, strides=[12544, 196, 14, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1], onnx_name=\"Conv_51\"](%onnx::Conv_382, %onnx::Conv_557, %onnx::Conv_558) # /opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py:454:0\n",
      "  %input.212 : Float(32, 64, 14, 14, strides=[12544, 196, 14, 1], requires_grad=1, device=cpu) = onnx::Add[onnx_name=\"Add_52\"](%input.188, %onnx::Add_556) # /opt/conda/lib/python3.7/site-packages/torch/nn/quantized/modules/functional_modules.py:43:0\n",
      "  %input.220 : Float(32, 384, 14, 14, strides=[75264, 196, 14, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1], onnx_name=\"Conv_53\"](%input.212, %onnx::Conv_560, %onnx::Conv_561) # /opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py:454:0\n",
      "  %onnx::Conv_388 : Float(32, 384, 14, 14, strides=[75264, 196, 14, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"Relu_54\"](%input.220) # /opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1455:0\n",
      "  %input.228 : Float(32, 384, 14, 14, strides=[75264, 196, 14, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=384, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"Conv_55\"](%onnx::Conv_388, %onnx::Conv_563, %onnx::Conv_564) # /opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py:454:0\n",
      "  %onnx::Conv_391 : Float(32, 384, 14, 14, strides=[75264, 196, 14, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"Relu_56\"](%input.228) # /opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1455:0\n",
      "  %onnx::Add_565 : Float(32, 64, 14, 14, strides=[12544, 196, 14, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1], onnx_name=\"Conv_57\"](%onnx::Conv_391, %onnx::Conv_566, %onnx::Conv_567) # /opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py:454:0\n",
      "  %input.236 : Float(32, 64, 14, 14, strides=[12544, 196, 14, 1], requires_grad=1, device=cpu) = onnx::Add[onnx_name=\"Add_58\"](%input.212, %onnx::Add_565) # /opt/conda/lib/python3.7/site-packages/torch/nn/quantized/modules/functional_modules.py:43:0\n",
      "  %input.244 : Float(32, 384, 14, 14, strides=[75264, 196, 14, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1], onnx_name=\"Conv_59\"](%input.236, %onnx::Conv_569, %onnx::Conv_570) # /opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py:454:0\n",
      "  %onnx::Conv_397 : Float(32, 384, 14, 14, strides=[75264, 196, 14, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"Relu_60\"](%input.244) # /opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1455:0\n",
      "  %input.252 : Float(32, 384, 14, 14, strides=[75264, 196, 14, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=384, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"Conv_61\"](%onnx::Conv_397, %onnx::Conv_572, %onnx::Conv_573) # /opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py:454:0\n",
      "  %onnx::Conv_400 : Float(32, 384, 14, 14, strides=[75264, 196, 14, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"Relu_62\"](%input.252) # /opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1455:0\n",
      "  %input.260 : Float(32, 96, 14, 14, strides=[18816, 196, 14, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1], onnx_name=\"Conv_63\"](%onnx::Conv_400, %onnx::Conv_575, %onnx::Conv_576) # /opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py:454:0\n",
      "  %input.268 : Float(32, 576, 14, 14, strides=[112896, 196, 14, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1], onnx_name=\"Conv_64\"](%input.260, %onnx::Conv_578, %onnx::Conv_579) # /opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py:454:0\n",
      "  %onnx::Conv_405 : Float(32, 576, 14, 14, strides=[112896, 196, 14, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"Relu_65\"](%input.268) # /opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1455:0\n",
      "  %input.276 : Float(32, 576, 14, 14, strides=[112896, 196, 14, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=576, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"Conv_66\"](%onnx::Conv_405, %onnx::Conv_581, %onnx::Conv_582) # /opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py:454:0\n",
      "  %onnx::Conv_408 : Float(32, 576, 14, 14, strides=[112896, 196, 14, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"Relu_67\"](%input.276) # /opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1455:0\n",
      "  %onnx::Add_583 : Float(32, 96, 14, 14, strides=[18816, 196, 14, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1], onnx_name=\"Conv_68\"](%onnx::Conv_408, %onnx::Conv_584, %onnx::Conv_585) # /opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py:454:0\n",
      "  %input.284 : Float(32, 96, 14, 14, strides=[18816, 196, 14, 1], requires_grad=1, device=cpu) = onnx::Add[onnx_name=\"Add_69\"](%input.260, %onnx::Add_583) # /opt/conda/lib/python3.7/site-packages/torch/nn/quantized/modules/functional_modules.py:43:0\n",
      "  %input.292 : Float(32, 576, 14, 14, strides=[112896, 196, 14, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1], onnx_name=\"Conv_70\"](%input.284, %onnx::Conv_587, %onnx::Conv_588) # /opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py:454:0\n",
      "  %onnx::Conv_414 : Float(32, 576, 14, 14, strides=[112896, 196, 14, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"Relu_71\"](%input.292) # /opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1455:0\n",
      "  %input.300 : Float(32, 576, 14, 14, strides=[112896, 196, 14, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=576, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"Conv_72\"](%onnx::Conv_414, %onnx::Conv_590, %onnx::Conv_591) # /opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py:454:0\n",
      "  %onnx::Conv_417 : Float(32, 576, 14, 14, strides=[112896, 196, 14, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"Relu_73\"](%input.300) # /opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1455:0\n",
      "  %onnx::Add_592 : Float(32, 96, 14, 14, strides=[18816, 196, 14, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1], onnx_name=\"Conv_74\"](%onnx::Conv_417, %onnx::Conv_593, %onnx::Conv_594) # /opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py:454:0\n",
      "  %input.308 : Float(32, 96, 14, 14, strides=[18816, 196, 14, 1], requires_grad=1, device=cpu) = onnx::Add[onnx_name=\"Add_75\"](%input.284, %onnx::Add_592) # /opt/conda/lib/python3.7/site-packages/torch/nn/quantized/modules/functional_modules.py:43:0\n",
      "  %onnx::Resize_424 : Float(0, strides=[1], device=cpu) = onnx::Constant[value=[ CPUFloatType{0} ], onnx_name=\"Constant_76\"]() # /opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:3938:0\n",
      "  %up_out : Float(32, 96, 28, 28, strides=[75264, 784, 28, 1], requires_grad=1, device=cpu) = onnx::Resize[coordinate_transformation_mode=\"pytorch_half_pixel\", cubic_coeff_a=-0.75, mode=\"linear\", nearest_mode=\"floor\", onnx_name=\"Resize_77\"](%input.308, %onnx::Resize_424, %onnx::Resize_631) # /opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:3938:0\n",
      "  %input.312 : Float(32, 128, 28, 28, strides=[100352, 784, 28, 1], requires_grad=1, device=cpu) = onnx::Concat[axis=1, onnx_name=\"Concat_78\"](%up_out, %input.140) # /opt/conda/lib/python3.7/site-packages/torch/nn/quantized/modules/functional_modules.py:69:0\n",
      "  %input.320 : Float(32, 19, 28, 28, strides=[14896, 784, 28, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1], onnx_name=\"Conv_79\"](%input.312, %onnx::Conv_596, %onnx::Conv_597) # /opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py:454:0\n",
      "  %onnx::Conv_429 : Float(32, 19, 28, 28, strides=[14896, 784, 28, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"Relu_80\"](%input.320) # /opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1455:0\n",
      "  %input.328 : Float(32, 19, 28, 28, strides=[14896, 784, 28, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=19, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"Conv_81\"](%onnx::Conv_429, %onnx::Conv_599, %onnx::Conv_600) # /opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py:454:0\n",
      "  %onnx::Conv_432 : Float(32, 19, 28, 28, strides=[14896, 784, 28, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"Relu_82\"](%input.328) # /opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1455:0\n",
      "  %input.336 : Float(32, 64, 28, 28, strides=[50176, 784, 28, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1], onnx_name=\"Conv_83\"](%onnx::Conv_432, %onnx::Conv_602, %onnx::Conv_603) # /opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py:454:0\n",
      "  %onnx::Resize_438 : Float(0, strides=[1], device=cpu) = onnx::Constant[value=[ CPUFloatType{0} ], onnx_name=\"Constant_84\"]() # /opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:3938:0\n",
      "  %up_out.3 : Float(32, 64, 56, 56, strides=[200704, 3136, 56, 1], requires_grad=1, device=cpu) = onnx::Resize[coordinate_transformation_mode=\"pytorch_half_pixel\", cubic_coeff_a=-0.75, mode=\"linear\", nearest_mode=\"floor\", onnx_name=\"Resize_85\"](%input.336, %onnx::Resize_438, %onnx::Resize_632) # /opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:3938:0\n",
      "  %input.340 : Float(32, 88, 56, 56, strides=[275968, 3136, 56, 1], requires_grad=1, device=cpu) = onnx::Concat[axis=1, onnx_name=\"Concat_86\"](%up_out.3, %input.68) # /opt/conda/lib/python3.7/site-packages/torch/nn/quantized/modules/functional_modules.py:69:0\n",
      "  %input.348 : Float(32, 13, 56, 56, strides=[40768, 3136, 56, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1], onnx_name=\"Conv_87\"](%input.340, %onnx::Conv_605, %onnx::Conv_606) # /opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py:454:0\n",
      "  %onnx::Conv_443 : Float(32, 13, 56, 56, strides=[40768, 3136, 56, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"Relu_88\"](%input.348) # /opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1455:0\n",
      "  %input.356 : Float(32, 13, 56, 56, strides=[40768, 3136, 56, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=13, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"Conv_89\"](%onnx::Conv_443, %onnx::Conv_608, %onnx::Conv_609) # /opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py:454:0\n",
      "  %onnx::Conv_446 : Float(32, 13, 56, 56, strides=[40768, 3136, 56, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"Relu_90\"](%input.356) # /opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1455:0\n",
      "  %input.364 : Float(32, 44, 56, 56, strides=[137984, 3136, 56, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1], onnx_name=\"Conv_91\"](%onnx::Conv_446, %onnx::Conv_611, %onnx::Conv_612) # /opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py:454:0\n",
      "  %onnx::Resize_452 : Float(0, strides=[1], device=cpu) = onnx::Constant[value=[ CPUFloatType{0} ], onnx_name=\"Constant_92\"]() # /opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:3938:0\n",
      "  %up_out.7 : Float(32, 44, 112, 112, strides=[551936, 12544, 112, 1], requires_grad=1, device=cpu) = onnx::Resize[coordinate_transformation_mode=\"pytorch_half_pixel\", cubic_coeff_a=-0.75, mode=\"linear\", nearest_mode=\"floor\", onnx_name=\"Resize_93\"](%input.364, %onnx::Resize_452, %onnx::Resize_633) # /opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:3938:0\n",
      "  %input.368 : Float(32, 60, 112, 112, strides=[752640, 12544, 112, 1], requires_grad=1, device=cpu) = onnx::Concat[axis=1, onnx_name=\"Concat_94\"](%up_out.7, %input.20) # /opt/conda/lib/python3.7/site-packages/torch/nn/quantized/modules/functional_modules.py:69:0\n",
      "  %input.376 : Float(32, 9, 112, 112, strides=[112896, 12544, 112, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1], onnx_name=\"Conv_95\"](%input.368, %onnx::Conv_614, %onnx::Conv_615) # /opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py:454:0\n",
      "  %onnx::Conv_457 : Float(32, 9, 112, 112, strides=[112896, 12544, 112, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"Relu_96\"](%input.376) # /opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1455:0\n",
      "  %input.384 : Float(32, 9, 112, 112, strides=[112896, 12544, 112, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=9, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"Conv_97\"](%onnx::Conv_457, %onnx::Conv_617, %onnx::Conv_618) # /opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py:454:0\n",
      "  %onnx::Conv_460 : Float(32, 9, 112, 112, strides=[112896, 12544, 112, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"Relu_98\"](%input.384) # /opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1455:0\n",
      "  %input.392 : Float(32, 30, 112, 112, strides=[376320, 12544, 112, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1], onnx_name=\"Conv_99\"](%onnx::Conv_460, %onnx::Conv_620, %onnx::Conv_621) # /opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py:454:0\n",
      "  %onnx::Resize_466 : Float(0, strides=[1], device=cpu) = onnx::Constant[value=[ CPUFloatType{0} ], onnx_name=\"Constant_100\"]() # /opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:3938:0\n",
      "  %up_out.11 : Float(32, 30, 224, 224, strides=[1505280, 50176, 224, 1], requires_grad=1, device=cpu) = onnx::Resize[coordinate_transformation_mode=\"pytorch_half_pixel\", cubic_coeff_a=-0.75, mode=\"linear\", nearest_mode=\"floor\", onnx_name=\"Resize_101\"](%input.392, %onnx::Resize_466, %onnx::Resize_634) # /opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:3938:0\n",
      "  %input.396 : Float(32, 33, 224, 224, strides=[1655808, 50176, 224, 1], requires_grad=1, device=cpu) = onnx::Concat[axis=1, onnx_name=\"Concat_102\"](%up_out.11, %input.1) # /opt/conda/lib/python3.7/site-packages/torch/nn/quantized/modules/functional_modules.py:69:0\n",
      "  %input.404 : Float(32, 5, 224, 224, strides=[250880, 50176, 224, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1], onnx_name=\"Conv_103\"](%input.396, %onnx::Conv_623, %onnx::Conv_624) # /opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py:454:0\n",
      "  %onnx::Conv_471 : Float(32, 5, 224, 224, strides=[250880, 50176, 224, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"Relu_104\"](%input.404) # /opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1455:0\n",
      "  %input.412 : Float(32, 5, 224, 224, strides=[250880, 50176, 224, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=5, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"Conv_105\"](%onnx::Conv_471, %onnx::Conv_626, %onnx::Conv_627) # /opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py:454:0\n",
      "  %onnx::Conv_474 : Float(32, 5, 224, 224, strides=[250880, 50176, 224, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"Relu_106\"](%input.412) # /opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1455:0\n",
      "  %input.420 : Float(32, 16, 224, 224, strides=[802816, 50176, 224, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1], onnx_name=\"Conv_107\"](%onnx::Conv_474, %onnx::Conv_629, %onnx::Conv_630) # /opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py:454:0\n",
      "  %477 : Float(32, 1, 224, 224, strides=[50176, 50176, 224, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1], onnx_name=\"Conv_108\"](%input.420, %decode1.conv.0.weight, %decode1.conv.0.bias) # /opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py:454:0\n",
      "  return (%477)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# export the model to ONNX\n",
    "torch.onnx.export(model, dummy_input, \"m2unet.onnx\", verbose=True, opset_version=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d4d634af-1a51-4952-b9c0-9e4ec1b201da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "&&&& RUNNING TensorRT.trtexec [TensorRT v8003] # trtexec --onnx=m2unet.onnx --saveEngine=m2unet_engine_pytorch.trt --explicitBatch\n",
      "[12/15/2022-11:30:39] [I] === Model Options ===\n",
      "[12/15/2022-11:30:39] [I] Format: ONNX\n",
      "[12/15/2022-11:30:39] [I] Model: m2unet.onnx\n",
      "[12/15/2022-11:30:39] [I] Output:\n",
      "[12/15/2022-11:30:39] [I] === Build Options ===\n",
      "[12/15/2022-11:30:39] [I] Max batch: explicit\n",
      "[12/15/2022-11:30:39] [I] Workspace: 16 MiB\n",
      "[12/15/2022-11:30:39] [I] minTiming: 1\n",
      "[12/15/2022-11:30:39] [I] avgTiming: 8\n",
      "[12/15/2022-11:30:39] [I] Precision: FP32\n",
      "[12/15/2022-11:30:39] [I] Calibration: \n",
      "[12/15/2022-11:30:39] [I] Refit: Disabled\n",
      "[12/15/2022-11:30:39] [I] Sparsity: Disabled\n",
      "[12/15/2022-11:30:39] [I] Safe mode: Disabled\n",
      "[12/15/2022-11:30:39] [I] Restricted mode: Disabled\n",
      "[12/15/2022-11:30:39] [I] Save engine: m2unet_engine_pytorch.trt\n",
      "[12/15/2022-11:30:39] [I] Load engine: \n",
      "[12/15/2022-11:30:39] [I] NVTX verbosity: 0\n",
      "[12/15/2022-11:30:39] [I] Tactic sources: Using default tactic sources\n",
      "[12/15/2022-11:30:39] [I] timingCacheMode: local\n",
      "[12/15/2022-11:30:39] [I] timingCacheFile: \n",
      "[12/15/2022-11:30:39] [I] Input(s)s format: fp32:CHW\n",
      "[12/15/2022-11:30:39] [I] Output(s)s format: fp32:CHW\n",
      "[12/15/2022-11:30:39] [I] Input build shapes: model\n",
      "[12/15/2022-11:30:39] [I] Input calibration shapes: model\n",
      "[12/15/2022-11:30:39] [I] === System Options ===\n",
      "[12/15/2022-11:30:39] [I] Device: 0\n",
      "[12/15/2022-11:30:39] [I] DLACore: \n",
      "[12/15/2022-11:30:39] [I] Plugins:\n",
      "[12/15/2022-11:30:39] [I] === Inference Options ===\n",
      "[12/15/2022-11:30:39] [I] Batch: Explicit\n",
      "[12/15/2022-11:30:39] [I] Input inference shapes: model\n",
      "[12/15/2022-11:30:39] [I] Iterations: 10\n",
      "[12/15/2022-11:30:39] [I] Duration: 3s (+ 200ms warm up)\n",
      "[12/15/2022-11:30:39] [I] Sleep time: 0ms\n",
      "[12/15/2022-11:30:39] [I] Streams: 1\n",
      "[12/15/2022-11:30:39] [I] ExposeDMA: Disabled\n",
      "[12/15/2022-11:30:39] [I] Data transfers: Enabled\n",
      "[12/15/2022-11:30:39] [I] Spin-wait: Disabled\n",
      "[12/15/2022-11:30:39] [I] Multithreading: Disabled\n",
      "[12/15/2022-11:30:39] [I] CUDA Graph: Disabled\n",
      "[12/15/2022-11:30:39] [I] Separate profiling: Disabled\n",
      "[12/15/2022-11:30:39] [I] Time Deserialize: Disabled\n",
      "[12/15/2022-11:30:39] [I] Time Refit: Disabled\n",
      "[12/15/2022-11:30:39] [I] Skip inference: Disabled\n",
      "[12/15/2022-11:30:39] [I] Inputs:\n",
      "[12/15/2022-11:30:39] [I] === Reporting Options ===\n",
      "[12/15/2022-11:30:39] [I] Verbose: Disabled\n",
      "[12/15/2022-11:30:39] [I] Averages: 10 inferences\n",
      "[12/15/2022-11:30:39] [I] Percentile: 99\n",
      "[12/15/2022-11:30:39] [I] Dump refittable layers:Disabled\n",
      "[12/15/2022-11:30:39] [I] Dump output: Disabled\n",
      "[12/15/2022-11:30:39] [I] Profile: Disabled\n",
      "[12/15/2022-11:30:39] [I] Export timing to JSON file: \n",
      "[12/15/2022-11:30:39] [I] Export output to JSON file: \n",
      "[12/15/2022-11:30:39] [I] Export profile to JSON file: \n",
      "[12/15/2022-11:30:39] [I] \n",
      "[12/15/2022-11:30:39] [I] === Device Information ===\n",
      "[12/15/2022-11:30:39] [I] Selected Device: NVIDIA A100-SXM4-40GB\n",
      "[12/15/2022-11:30:39] [I] Compute Capability: 8.0\n",
      "[12/15/2022-11:30:39] [I] SMs: 108\n",
      "[12/15/2022-11:30:39] [I] Compute Clock Rate: 1.41 GHz\n",
      "[12/15/2022-11:30:39] [I] Device Global Memory: 40354 MiB\n",
      "[12/15/2022-11:30:39] [I] Shared Memory per SM: 164 KiB\n",
      "[12/15/2022-11:30:39] [I] Memory Bus Width: 5120 bits (ECC enabled)\n",
      "[12/15/2022-11:30:39] [I] Memory Clock Rate: 1.215 GHz\n",
      "[12/15/2022-11:30:39] [I] \n",
      "[12/15/2022-11:30:39] [I] TensorRT version: 8003\n",
      "[12/15/2022-11:30:39] [I] [TRT] [MemUsageChange] Init CUDA: CPU +501, GPU +0, now: CPU 508, GPU 9014 (MiB)\n",
      "[12/15/2022-11:30:39] [I] Start parsing network model\n",
      "[12/15/2022-11:30:39] [I] [TRT] ----------------------------------------------------------------\n",
      "[12/15/2022-11:30:39] [I] [TRT] Input filename:   m2unet.onnx\n",
      "[12/15/2022-11:30:39] [I] [TRT] ONNX IR version:  0.0.7\n",
      "[12/15/2022-11:30:39] [I] [TRT] Opset version:    12\n",
      "[12/15/2022-11:30:39] [I] [TRT] Producer name:    pytorch\n",
      "[12/15/2022-11:30:39] [I] [TRT] Producer version: 1.12.1\n",
      "[12/15/2022-11:30:39] [I] [TRT] Domain:           \n",
      "[12/15/2022-11:30:39] [I] [TRT] Model version:    0\n",
      "[12/15/2022-11:30:39] [I] [TRT] Doc string:       \n",
      "[12/15/2022-11:30:39] [I] [TRT] ----------------------------------------------------------------\n",
      "[12/15/2022-11:30:39] [E] [TRT] ModelImporter.cpp:720: While parsing node number 85 [Resize -> \"up_out.3\"]:\n",
      "[12/15/2022-11:30:39] [E] [TRT] ModelImporter.cpp:721: --- Begin node ---\n",
      "[12/15/2022-11:30:39] [E] [TRT] ModelImporter.cpp:722: input: \"input.336\"\n",
      "input: \"onnx::Resize_438\"\n",
      "input: \"onnx::Resize_632\"\n",
      "output: \"up_out.3\"\n",
      "name: \"Resize_85\"\n",
      "op_type: \"Resize\"\n",
      "attribute {\n",
      "  name: \"coordinate_transformation_mode\"\n",
      "  s: \"pytorch_half_pixel\"\n",
      "  type: STRING\n",
      "}\n",
      "attribute {\n",
      "  name: \"cubic_coeff_a\"\n",
      "  f: -0.75\n",
      "  type: FLOAT\n",
      "}\n",
      "attribute {\n",
      "  name: \"mode\"\n",
      "  s: \"linear\"\n",
      "  type: STRING\n",
      "}\n",
      "attribute {\n",
      "  name: \"nearest_mode\"\n",
      "  s: \"floor\"\n",
      "  type: STRING\n",
      "}\n",
      "doc_string: \"/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py(3938): interpolate\\n/opt/conda/lib/python3.7/site-packages/torch/nn/modules/upsampling.py(154): forward\\n/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py(1118): _slow_forward\\n/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py(1130): _call_impl\\n/tmp/ipykernel_10610/3224468684.py(112): forward\\n/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py(1118): _slow_forward\\n/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py(1130): _call_impl\\n/tmp/ipykernel_10610/3224468684.py(240): forward\\n/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py(1118): _slow_forward\\n/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py(1130): _call_impl\\n/opt/conda/lib/python3.7/site-packages/torch/jit/_trace.py(118): wrapper\\n/opt/conda/lib/python3.7/site-packages/torch/jit/_trace.py(132): forward\\n/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py(1130): _call_impl\\n/opt/conda/lib/python3.7/site-packages/torch/jit/_trace.py(1175): _get_trace_graph\\n/opt/conda/lib/python3.7/site-packages/torch/onnx/utils.py(518): _trace_and_get_graph_from_model\\n/opt/conda/lib/python3.7/site-packages/torch/onnx/utils.py(602): _create_jit_graph\\n/opt/conda/lib/python3.7/site-packages/torch/onnx/utils.py(727): _model_to_graph\\n/opt/conda/lib/python3.7/site-packages/torch/onnx/utils.py(1084): _export\\n/opt/conda/lib/python3.7/site-packages/torch/onnx/utils.py(178): export\\n/opt/conda/lib/python3.7/site-packages/torch/onnx/__init__.py(365): export\\n/tmp/ipykernel_10610/1428375706.py(2): <module>\\n/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py(3552): run_code\\n/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py(3472): run_ast_nodes\\n/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py(3257): run_cell_async\\n/opt/conda/lib/python3.7/site-packages/IPython/core/async_helpers.py(78): _pseudo_sync_runner\\n/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py(3029): _run_cell\\n/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py(2975): run_cell\\n/opt/conda/lib/python3.7/site-packages/ipykernel/zmqshell.py(528): run_cell\\n/opt/conda/lib/python3.7/site-packages/ipykernel/ipkernel.py(387): do_execute\\n/opt/conda/lib/python3.7/site-packages/ipykernel/kernelbase.py(730): execute_request\\n/opt/conda/lib/python3.7/site-packages/ipykernel/kernelbase.py(406): dispatch_shell\\n/opt/conda/lib/python3.7/site-packages/ipykernel/kernelbase.py(499): process_one\\n/opt/conda/lib/python3.7/site-packages/ipykernel/kernelbase.py(510): dispatch_queue\\n/opt/conda/lib/python3.7/asyncio/events.py(88): _run\\n/opt/conda/lib/python3.7/asyncio/base_events.py(1786): _run_once\\n/opt/conda/lib/python3.7/asyncio/base_events.py(541): run_forever\\n/opt/conda/lib/python3.7/site-packages/tornado/platform/asyncio.py(215): start\\n/opt/conda/lib/python3.7/site-packages/ipykernel/kernelapp.py(712): start\\n/opt/conda/lib/python3.7/site-packages/traitlets/config/application.py(976): launch_instance\\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py(17): <module>\\n/opt/conda/lib/python3.7/runpy.py(85): _run_code\\n/opt/conda/lib/python3.7/runpy.py(193): _run_module_as_main\\n\"\n",
      "\n",
      "[12/15/2022-11:30:39] [E] [TRT] ModelImporter.cpp:723: --- End node ---\n",
      "[12/15/2022-11:30:39] [E] [TRT] ModelImporter.cpp:726: ERROR: builtin_op_importers.cpp:3422 In function importResize:\n",
      "[8] Assertion failed: scales.is_weights() && \"Resize scales must be an initializer!\"\n",
      "[12/15/2022-11:30:39] [E] Failed to parse onnx file\n",
      "[12/15/2022-11:30:39] [I] Finish parsing network model\n",
      "[12/15/2022-11:30:39] [E] Parsing model failed\n",
      "[12/15/2022-11:30:39] [E] Engine creation failed\n",
      "[12/15/2022-11:30:39] [E] Engine set up failed\n",
      "&&&& FAILED TensorRT.trtexec [TensorRT v8003] # trtexec --onnx=m2unet.onnx --saveEngine=m2unet_engine_pytorch.trt --explicitBatch\n"
     ]
    }
   ],
   "source": [
    "!trtexec --onnx=m2unet.onnx --saveEngine=m2unet_engine_pytorch.trt  --explicitBatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56d9e2d-b1ce-4106-b165-a57cd14a2092",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu110.m95",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m95"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
