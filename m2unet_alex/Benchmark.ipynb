{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d446acd8-7add-435f-9a86-0e7758f3d695",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The purpose of this file is to benchmark the speeds of different models before and after quantization\n",
    "'''\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d37477-d1b6-4d8b-bcae-84fcedc8a02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Original m2unet\n",
    "'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "def conv_bn(inp, oup, stride):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        nn.ReLU6(inplace=True)\n",
    "    )\n",
    "\n",
    "\n",
    "def conv_1x1_bn(inp, oup):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        nn.ReLU6(inplace=True)\n",
    "    )\n",
    "\n",
    "class InvertedResidual(nn.Module):\n",
    "    def __init__(self, inp, oup, stride, expand_ratio):\n",
    "        super(InvertedResidual, self).__init__()\n",
    "        self.stride = stride\n",
    "        assert stride in [1, 2]\n",
    "\n",
    "        hidden_dim = round(inp * expand_ratio)\n",
    "        self.use_res_connect = self.stride == 1 and inp == oup\n",
    "\n",
    "        if expand_ratio == 1:\n",
    "            # depthwise separable convolution block\n",
    "            self.conv = nn.Sequential(\n",
    "                # dw\n",
    "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.ReLU6(inplace=True),\n",
    "                # pw-linear\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "        else:\n",
    "            # Bottleneck with expansion layer\n",
    "            self.conv = nn.Sequential(\n",
    "                # pw\n",
    "                nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.ReLU6(inplace=True),\n",
    "                # dw\n",
    "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.ReLU6(inplace=True),\n",
    "                # pw-linear\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_res_connect:\n",
    "            return x + self.conv(x)\n",
    "        else:\n",
    "            return self.conv(x)\n",
    "        \n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    14 layers of MobileNetv2 as encoder part\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        block = InvertedResidual\n",
    "        interverted_residual_setting = [\n",
    "            # t, c, n, s\n",
    "            [1, 16, 1, 1],\n",
    "            [6, 24, 2, 2],\n",
    "            [6, 32, 3, 2],\n",
    "            [6, 64, 4, 2],\n",
    "            [6, 96, 3, 1],\n",
    "        ]\n",
    "        # Encoder Part\n",
    "        input_channel = 32 # number of input channels to first inverted (residual) block\n",
    "        self.layers = [conv_bn(3, 32, 2)]\n",
    "        # building inverted residual blocks\n",
    "        for t, c, n, s in interverted_residual_setting:\n",
    "            output_channel = c\n",
    "            for i in range(n):\n",
    "                if i == 0:\n",
    "                    self.layers.append(block(input_channel, output_channel, s, expand_ratio=t))\n",
    "                else:\n",
    "                    self.layers.append(block(input_channel, output_channel, 1, expand_ratio=t))\n",
    "                input_channel = output_channel\n",
    "        # make it nn.Sequential\n",
    "        self.layers = nn.Sequential(*self.layers)\n",
    "                \n",
    "class DecoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder block: upsample and concatenate with features maps from the encoder part\n",
    "    \"\"\"\n",
    "    def __init__(self,up_in_c,x_in_c,upsamplemode='bilinear',expand_ratio=0.15):\n",
    "        super().__init__()\n",
    "        self.upsample = nn.Upsample(scale_factor=2,mode=upsamplemode,align_corners=False) # H, W -> 2H, 2W\n",
    "        self.ir1 = InvertedResidual(up_in_c+x_in_c,(x_in_c + up_in_c) // 2,stride=1,expand_ratio=expand_ratio)\n",
    "\n",
    "    def forward(self,up_in,x_in):\n",
    "        up_out = self.upsample(up_in)\n",
    "        cat_x = torch.cat([up_out, x_in] , dim=1)\n",
    "        x = self.ir1(cat_x)\n",
    "        return x\n",
    "    \n",
    "class LastDecoderBlock(nn.Module):\n",
    "    def __init__(self,x_in_c,upsamplemode='bilinear',expand_ratio=0.15, output_channels=1, activation='linear'):\n",
    "        super().__init__()\n",
    "        self.upsample = nn.Upsample(scale_factor=2,mode=upsamplemode,align_corners=False) # H, W -> 2H, 2W\n",
    "        self.ir1 = InvertedResidual(x_in_c,16,stride=1,expand_ratio=expand_ratio)\n",
    "        layers =  [\n",
    "            nn.Conv2d(16, output_channels, 1, 1, 0, bias=True),\n",
    "        ]\n",
    "        if activation == 'sigmoid':\n",
    "            layers.append(nn.Sigmoid())\n",
    "        elif activation == 'softmax':\n",
    "            layers.append(nn.Softmax(dim=1))\n",
    "        elif activation == 'linear' or activation is None:\n",
    "            pass\n",
    "        else:\n",
    "            raise NotImplementedError('Activation {} not implemented'.format(activation))\n",
    "        self.conv = nn.Sequential(\n",
    "           *layers\n",
    "        )\n",
    "\n",
    "    def forward(self,up_in,x_in):\n",
    "        up_out = self.upsample(up_in)\n",
    "        cat_x = torch.cat([up_out, x_in] , dim=1)\n",
    "        x = self.ir1(cat_x)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "    \n",
    "class M2UNet(nn.Module):\n",
    "        def __init__(self,encoder,upsamplemode='bilinear',output_channels=1, activation=\"linear\", expand_ratio=0.15):\n",
    "            super(M2UNet,self).__init__()\n",
    "            encoder = list(encoder.children())[0]\n",
    "            # Encoder\n",
    "            self.conv1 = encoder[0:2]\n",
    "            self.conv2 = encoder[2:4]\n",
    "            self.conv3 = encoder[4:7]\n",
    "            self.conv4 = encoder[7:14]\n",
    "            # Decoder\n",
    "            self.decode4 = DecoderBlock(96,32,upsamplemode,expand_ratio)\n",
    "            self.decode3 = DecoderBlock(64,24,upsamplemode,expand_ratio)\n",
    "            self.decode2 = DecoderBlock(44,16,upsamplemode,expand_ratio)\n",
    "            self.decode1 = LastDecoderBlock(33,upsamplemode,expand_ratio, output_channels=output_channels, activation=activation)\n",
    "            # initilaize weights \n",
    "            self._init_params()\n",
    "\n",
    "        def _init_params(self):\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, nn.Conv2d):\n",
    "                    n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                    m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                    if m.bias is not None:\n",
    "                        m.bias.data.zero_()\n",
    "                elif isinstance(m, nn.BatchNorm2d):\n",
    "                    m.weight.data.fill_(1)\n",
    "                    m.bias.data.zero_()\n",
    "                elif isinstance(m, nn.Linear):\n",
    "                    n = m.weight.size(1)\n",
    "                    m.weight.data.normal_(0, 0.01)\n",
    "                    m.bias.data.zero_()\n",
    "            \n",
    "        \n",
    "        \n",
    "        def forward(self,x):\n",
    "            conv1 = self.conv1(x)\n",
    "            conv2 = self.conv2(conv1)\n",
    "            conv3 = self.conv3(conv2)\n",
    "            conv4 = self.conv4(conv3)\n",
    "            decode4 = self.decode4(conv4,conv3)\n",
    "            decode3 = self.decode3(decode4,conv2)\n",
    "            decode2 = self.decode2(decode3,conv1)\n",
    "            decode1 = self.decode1(decode2,x)\n",
    "            return decode1\n",
    "        \n",
    "class M2UNet_q(nn.Module):\n",
    "        def __init__(self,encoder,upsamplemode='bilinear',output_channels=1, activation=\"linear\", expand_ratio=0.15):\n",
    "            super(M2UNet_q,self).__init__()\n",
    "            encoder = list(encoder.children())[0]\n",
    "            # Encoder\n",
    "            self.quant = torch.quantization.QuantStub()\n",
    "\n",
    "            self.conv1 = encoder[0:2]\n",
    "            self.conv2 = encoder[2:4]\n",
    "            self.conv3 = encoder[4:7]\n",
    "            self.conv4 = encoder[7:14]\n",
    "            # Decoder\n",
    "            self.decode4 = DecoderBlock(96,32,upsamplemode,expand_ratio)\n",
    "            self.decode3 = DecoderBlock(64,24,upsamplemode,expand_ratio)\n",
    "            self.decode2 = DecoderBlock(44,16,upsamplemode,expand_ratio)\n",
    "            self.decode1 = LastDecoderBlock(33,upsamplemode,expand_ratio, output_channels=output_channels, activation=activation)\n",
    "            \n",
    "            self.dequant = torch.quantization.DeQuantStub()\n",
    "\n",
    "            # initilaize weights \n",
    "            self._init_params()\n",
    "\n",
    "        def _init_params(self):\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, nn.Conv2d):\n",
    "                    n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                    m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                    if m.bias is not None:\n",
    "                        m.bias.data.zero_()\n",
    "                elif isinstance(m, nn.BatchNorm2d):\n",
    "                    m.weight.data.fill_(1)\n",
    "                    m.bias.data.zero_()\n",
    "                elif isinstance(m, nn.Linear):\n",
    "                    n = m.weight.size(1)\n",
    "                    m.weight.data.normal_(0, 0.01)\n",
    "                    m.bias.data.zero_()\n",
    "            \n",
    "        \n",
    "        \n",
    "        def forward(self,x):\n",
    "            x = self.quant(x)\n",
    "\n",
    "            conv1 = self.conv1(x)\n",
    "            conv2 = self.conv2(conv1)\n",
    "            conv3 = self.conv3(conv2)\n",
    "            conv4 = self.conv4(conv3)\n",
    "            decode4 = self.decode4(conv4,conv3)\n",
    "            decode3 = self.decode3(decode4,conv2)\n",
    "            decode2 = self.decode2(decode3,conv1)\n",
    "            decode1 = self.decode1(decode2,x)\n",
    "            \n",
    "            res = self.dequant(decode1)\n",
    "\n",
    "            \n",
    "            return res\n",
    "        \n",
    "def m2unet(output_channels=1,expand_ratio=0.15, activation=\"linear\", **kwargs):\n",
    "    encoder = Encoder()\n",
    "    model = M2UNet(encoder,upsamplemode='bilinear',expand_ratio=expand_ratio, output_channels=output_channels, activation=activation)\n",
    "    return model\n",
    "\n",
    "def m2unet_q(output_channels=1,expand_ratio=0.15, activation=\"linear\", **kwargs):\n",
    "    encoder = Encoder()\n",
    "    model = M2UNet_q(encoder,upsamplemode='bilinear',expand_ratio=expand_ratio, output_channels=output_channels, activation=activation)\n",
    "    return model\n",
    "\n",
    "\n",
    "## Defines Unet\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from glob import glob\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transform\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "class Convblock(nn.Module):\n",
    "    \n",
    "      def __init__(self,input_channel,output_channel,kernal=3,stride=1,padding=1):\n",
    "            \n",
    "        super().__init__()\n",
    "        self.convblock = nn.Sequential(\n",
    "            nn.Conv2d(input_channel,output_channel,kernal,stride,padding),\n",
    "            nn.BatchNorm2d(output_channel),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(output_channel,output_channel,kernal),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "    \n",
    "\n",
    "      def forward(self,x):\n",
    "        x = self.convblock(x)\n",
    "        return x\n",
    "    \n",
    "class UNet(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_channel,retain=True):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = Convblock(input_channel,32)\n",
    "        self.conv2 = Convblock(32,64)\n",
    "        self.conv3 = Convblock(64,128)\n",
    "        self.conv4 = Convblock(128,256)\n",
    "        self.neck = nn.Conv2d(256,512,3,1)\n",
    "        self.upconv4 = nn.ConvTranspose2d(512,256,3,2,0,1)\n",
    "        self.dconv4 = Convblock(512,256)\n",
    "        self.upconv3 = nn.ConvTranspose2d(256,128,3,2,0,1)\n",
    "        self.dconv3 = Convblock(256,128)\n",
    "        self.upconv2 = nn.ConvTranspose2d(128,64,3,2,0,1)\n",
    "        self.dconv2 = Convblock(128,64)\n",
    "        self.upconv1 = nn.ConvTranspose2d(64,32,3,2,0,1)\n",
    "        self.dconv1 = Convblock(64,32)\n",
    "        self.out = nn.Conv2d(32,1,1,1)\n",
    "        self.retain = retain\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        # Encoder Network\n",
    "        \n",
    "        # Conv down 1\n",
    "        conv1 = self.conv1(x)\n",
    "        pool1 = F.max_pool2d(conv1,kernel_size=2,stride=2)\n",
    "        # Conv down 2\n",
    "        conv2 = self.conv2(pool1)\n",
    "        pool2 = F.max_pool2d(conv2,kernel_size=2,stride=2)\n",
    "        # Conv down 3\n",
    "        conv3 = self.conv3(pool2)\n",
    "        pool3 = F.max_pool2d(conv3,kernel_size=2,stride=2)\n",
    "        # Conv down 4\n",
    "        conv4 = self.conv4(pool3)\n",
    "        pool4 = F.max_pool2d(conv4,kernel_size=2,stride=2)\n",
    "\n",
    "        # BottelNeck\n",
    "        neck = self.neck(pool4)\n",
    "        \n",
    "        # Decoder Network\n",
    "        \n",
    "        # Upconv 1\n",
    "        upconv4 = self.upconv4(neck)\n",
    "        croped = self.crop(conv4,upconv4)\n",
    "        # Making the skip connection 1\n",
    "        dconv4 = self.dconv4(torch.cat([upconv4,croped],1))\n",
    "        # Upconv 2\n",
    "        upconv3 = self.upconv3(dconv4)\n",
    "        croped = self.crop(conv3,upconv3)\n",
    "        # Making the skip connection 2\n",
    "        dconv3 = self.dconv3(torch.cat([upconv3,croped],1))\n",
    "        # Upconv 3\n",
    "        upconv2 = self.upconv2(dconv3)\n",
    "        croped = self.crop(conv2,upconv2)\n",
    "        # Making the skip connection 3\n",
    "        dconv2 = self.dconv2(torch.cat([upconv2,croped],1))\n",
    "        # Upconv 4\n",
    "        upconv1 = self.upconv1(dconv2)\n",
    "        croped = self.crop(conv1,upconv1)\n",
    "        # Making the skip connection 4\n",
    "        dconv1 = self.dconv1(torch.cat([upconv1,croped],1))\n",
    "        # Output Layer\n",
    "        out = self.out(dconv1)\n",
    "        \n",
    "        if self.retain == True:\n",
    "            out = F.interpolate(out,list(x.shape)[2:])\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def crop(self,input_tensor,target_tensor):\n",
    "        # For making the size of the encoder conv layer and the decoder Conv layer same\n",
    "        _,_,H,W = target_tensor.shape\n",
    "        return transform.CenterCrop([H,W])(input_tensor)\n",
    "    \n",
    "class UNet_q(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_channel,retain=True):\n",
    "\n",
    "        super().__init__()\n",
    "        \n",
    "        self.quant = torch.quantization.QuantStub()\n",
    "\n",
    "        self.conv1 = Convblock(input_channel,32)\n",
    "        self.conv2 = Convblock(32,64)\n",
    "        self.conv3 = Convblock(64,128)\n",
    "        self.conv4 = Convblock(128,256)\n",
    "        self.neck = nn.Conv2d(256,512,3,1)\n",
    "        self.upconv4 = nn.ConvTranspose2d(512,256,3,2,0,1)\n",
    "        self.dconv4 = Convblock(512,256)\n",
    "        self.upconv3 = nn.ConvTranspose2d(256,128,3,2,0,1)\n",
    "        self.dconv3 = Convblock(256,128)\n",
    "        self.upconv2 = nn.ConvTranspose2d(128,64,3,2,0,1)\n",
    "        self.dconv2 = Convblock(128,64)\n",
    "        self.upconv1 = nn.ConvTranspose2d(64,32,3,2,0,1)\n",
    "        self.dconv1 = Convblock(64,32)\n",
    "        self.out = nn.Conv2d(32,1,1,1)\n",
    "        self.retain = retain\n",
    "        \n",
    "        self.dequant = torch.quantization.DeQuantStub()\n",
    "\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        # Encoder Network\n",
    "        \n",
    "        # Conv down 1\n",
    "        x = self.quant(x)\n",
    "        conv1 = self.conv1(x)\n",
    "        pool1 = F.max_pool2d(conv1,kernel_size=2,stride=2)\n",
    "        # Conv down 2\n",
    "        conv2 = self.conv2(pool1)\n",
    "        pool2 = F.max_pool2d(conv2,kernel_size=2,stride=2)\n",
    "        # Conv down 3\n",
    "        conv3 = self.conv3(pool2)\n",
    "        pool3 = F.max_pool2d(conv3,kernel_size=2,stride=2)\n",
    "        # Conv down 4\n",
    "        conv4 = self.conv4(pool3)\n",
    "        pool4 = F.max_pool2d(conv4,kernel_size=2,stride=2)\n",
    "\n",
    "        # BottelNeck\n",
    "        neck = self.neck(pool4)\n",
    "        \n",
    "        # Decoder Network\n",
    "        \n",
    "        # Upconv 1\n",
    "        upconv4 = self.upconv4(neck)\n",
    "        croped = self.crop(conv4,upconv4)\n",
    "        # Making the skip connection 1\n",
    "        dconv4 = self.dconv4(torch.cat([upconv4,croped],1))\n",
    "        # Upconv 2\n",
    "        upconv3 = self.upconv3(dconv4)\n",
    "        croped = self.crop(conv3,upconv3)\n",
    "        # Making the skip connection 2\n",
    "        dconv3 = self.dconv3(torch.cat([upconv3,croped],1))\n",
    "        # Upconv 3\n",
    "        upconv2 = self.upconv2(dconv3)\n",
    "        croped = self.crop(conv2,upconv2)\n",
    "        # Making the skip connection 3\n",
    "        dconv2 = self.dconv2(torch.cat([upconv2,croped],1))\n",
    "        # Upconv 4\n",
    "        upconv1 = self.upconv1(dconv2)\n",
    "        croped = self.crop(conv1,upconv1)\n",
    "        # Making the skip connection 4\n",
    "        dconv1 = self.dconv1(torch.cat([upconv1,croped],1))\n",
    "        # Output Layer\n",
    "        out = self.out(dconv1)\n",
    "        \n",
    "        out = self.dequant(dconv1)\n",
    "        \n",
    "        if self.retain == True:\n",
    "            out = F.interpolate(out,list(x.shape)[2:])\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def crop(self,input_tensor,target_tensor):\n",
    "        # For making the size of the encoder conv layer and the decoder Conv layer same\n",
    "        _,_,H,W = target_tensor.shape\n",
    "        return transform.CenterCrop([H,W])(input_tensor)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41a4a8c-a382-4d9d-b571-99fbcb5f3d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define Model\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model = UNet(3).float().to(device)\n",
    "\n",
    "# Warm up\n",
    "inp = torch.randn([1, 3, 512, 512]).to(device)\n",
    "output = model(inp)\n",
    "\n",
    "## Inference speed\n",
    "trials = 100\n",
    "total_time = 0\n",
    "for i in range(trials):\n",
    "    inp = torch.randn([1, 3, 512, 512]).to(device)\n",
    "    torch.cuda.synchronize()\n",
    "    start_epoch = time.time()\n",
    "    output = model(inp)\n",
    "    torch.cuda.synchronize()\n",
    "    end_epoch = time.time()\n",
    "    elapsed = end_epoch - start_epoch\n",
    "    total_time += elapsed\n",
    "    \n",
    "print('The average time per inference for unquantized unet is')\n",
    "print(total_time/trials)\n",
    "\n",
    "\n",
    "## Training speed\n",
    "\n",
    "lr = .0001 # Doesn't matter\n",
    "lossfunc = nn.MSELoss().cuda('0')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "trials = 100\n",
    "total_time = 0\n",
    "batch_size = 1\n",
    "for i in range(trials):\n",
    "    torch.cuda.synchronize()\n",
    "                             \n",
    "    inp = torch.randn([batch_size, 3, 512, 512]).to(device)\n",
    "    label = torch.randn([batch_size, 1, 512, 512]).to(device)\n",
    "    start_epoch = time.time()\n",
    "                                                    \n",
    "    output = model(inp)                         \n",
    "    loss = lossfunc(output,label)\n",
    "    loss.backward()\n",
    "    optimizer.step()              \n",
    "                             \n",
    "    torch.cuda.synchronize()\n",
    "    end_epoch = time.time()\n",
    "    elapsed = end_epoch - start_epoch\n",
    "    total_time += elapsed\n",
    "    \n",
    "print('The average time per training cycle (batch_size = 1) for unquantized unet is')\n",
    "print(total_time/trials)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdf011a-ee81-48dd-859a-692ed311c410",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define Model\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model = m2unet().float().to(device)\n",
    "\n",
    "# Warm up\n",
    "inp = torch.randn([1, 3, 512, 512]).to(device)\n",
    "output = model(inp)\n",
    "\n",
    "## Inference speed\n",
    "trials = 100\n",
    "total_time = 0\n",
    "for i in range(trials):\n",
    "    inp = torch.randn([1, 3, 512, 512]).to(device)\n",
    "    torch.cuda.synchronize()\n",
    "    start_epoch = time.time()\n",
    "    output = model(inp)\n",
    "    torch.cuda.synchronize()\n",
    "    end_epoch = time.time()\n",
    "    elapsed = end_epoch - start_epoch\n",
    "    total_time += elapsed\n",
    "    \n",
    "print('The average time per inference for unquantized m2unet is')\n",
    "print(total_time/trials)\n",
    "\n",
    "\n",
    "## Training speed\n",
    "\n",
    "lr = .0001 # Doesn't matter\n",
    "lossfunc = nn.MSELoss().cuda('0')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "trials = 100\n",
    "total_time = 0\n",
    "batch_size = 1\n",
    "for i in range(trials):\n",
    "    torch.cuda.synchronize()\n",
    "                             \n",
    "    inp = torch.randn([batch_size, 3, 512, 512]).to(device)\n",
    "    label = torch.randn([batch_size, 1, 512, 512]).to(device)\n",
    "    start_epoch = time.time()\n",
    "                                                    \n",
    "    output = model(inp)                         \n",
    "    loss = lossfunc(output,label)\n",
    "    loss.backward()\n",
    "    optimizer.step()              \n",
    "                             \n",
    "    torch.cuda.synchronize()\n",
    "    end_epoch = time.time()\n",
    "    elapsed = end_epoch - start_epoch\n",
    "    total_time += elapsed\n",
    "    \n",
    "print('The average time per training cycle (batch_size = 1) for unquantized m2unet is')\n",
    "print(total_time/trials)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcaa7833-0841-4e6f-aa30-2ebca65642de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "# create a model instance\n",
    "device = 'cpu'\n",
    "model_fp32 = m2unet_q(3).to(device)\n",
    "# model_fp32 = m2unet_q.to(device)\n",
    "\n",
    "# model must be set to eval mode for static quantization logic to work\n",
    "model_fp32.eval()\n",
    "\n",
    "# attach a global qconfig, which contains information about what kind\n",
    "# of observers to attach. Use 'fbgemm' for server inference and\n",
    "# 'qnnpack' for mobile inference. Other quantization configurations such\n",
    "# as selecting symmetric or assymetric quantization and MinMax or L2Norm\n",
    "# calibration techniques can be specified here.\n",
    "model_fp32.qconfig = torch.quantization.get_default_qconfig('qnnpack')\n",
    "\n",
    "# Fuse the activations to preceding layers, where applicable.\n",
    "# This needs to be done manually depending on the model architecture.\n",
    "# Common fusions include `conv + relu` and `conv + batchnorm + relu`\n",
    "# model_fp32_fused = torch.quantization.fuse_modules(model_fp32, [['conv1', 'conv2']])\n",
    "model_fp32_fused = model_fp32\n",
    "\n",
    "\n",
    "# Prepare the model for static quantization. This inserts observers in\n",
    "# the model that will observe activation tensors during calibration.\n",
    "model_fp32_prepared = torch.quantization.prepare(model_fp32_fused)\n",
    "\n",
    "\n",
    "# calibrate the prepared model to determine quantization parameters for activations\n",
    "# in a real world setting, the calibration would be done with a representative dataset\n",
    "input_fp32 = torch.randn(5, 3, 512, 512).to(device)\n",
    "model_fp32_prepared(input_fp32)\n",
    "\n",
    "# Convert the observed model to a quantized model. This does several things:\n",
    "# quantizes the weights, computes and stores the scale and bias value to be\n",
    "# used with each activation tensor, and replaces key operators with quantized\n",
    "# implementations.\n",
    "\n",
    "        # self.upconv4 = nn.ConvTranspose2d(512,256,3,2,0,1).\n",
    "        # self.dconv4 = Convblock(512,256)\n",
    "        # self.upconv3 = nn.ConvTranspose2d(256,128,3,2,0,1)\n",
    "        # self.dconv3 = Convblock(256,128)\n",
    "        # self.upconv2 = nn.ConvTranspose2d(128,64,3,2,0,1)\n",
    "        # self.dconv2 = Convblock(128,64)\n",
    "        # self.upconv1 = nn.ConvTranspose2d(64,32,3,2,0,1)\n",
    "        \n",
    "# model_fp32_prepared.upconv4.qconfig = None\n",
    "# model_fp32_prepared.upconv3.qconfig = None\n",
    "\n",
    "# model_fp32_prepared.upconv2.qconfig = None\n",
    "\n",
    "# model_fp32_prepared.upconv1.qconfig = None\n",
    "\n",
    "\n",
    "\n",
    "model_int8 = torch.quantization.convert(model_fp32_prepared)\n",
    "\n",
    "# run the model, relevant calculations will happen in int8\n",
    "# res = model_int8(input_fp32)\n",
    "\n",
    "# print(res)\n",
    "\n",
    "model_int8_cuda = model_int8.cuda(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5d5004-fd09-4a33-9e2f-d06882611d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model_int8_cuda)\n",
    "sample = torch.randn([1,3,512,512]).cuda(0)\n",
    "res = model_int8_cuda(sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bb06ac-43d0-4d2a-87ee-33cf79fdc7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.randn(1, 3, 512, 512).cuda(0)\n",
    "model = model_int8.cuda(0)\n",
    "res = model_int8(inp)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b33182-75c0-4853-9639-682e020d3808",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "inp = torch.randn(1, 3, 512, 512).to(device)\n",
    "model_int8 = model_int8.to(device)\n",
    "print(model_int8)\n",
    "result = model_int8(inp)\n",
    "# print(result)\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "start_epoch = time.time()\n",
    "\n",
    "result = model_int8(inp)\n",
    "\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "end_epoch = time.time()\n",
    "\n",
    "print(end_epoch - start_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82b2a65-773f-4f37-84ab-e4b7867e7b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model = model_int8.to(device)\n",
    "\n",
    "# Warm up\n",
    "inp = torch.randn([1, 3, 512, 512]).to(device)\n",
    "output = model(inp)\n",
    "\n",
    "## Inference speed\n",
    "trials = 100\n",
    "total_time = 0\n",
    "for i in range(trials):\n",
    "    inp = torch.randn([1, 3, 512, 512]).to(device)\n",
    "    torch.cuda.synchronize()\n",
    "    start_epoch = time.time()\n",
    "    output = model(inp)\n",
    "    torch.cuda.synchronize()\n",
    "    end_epoch = time.time()\n",
    "    elapsed = end_epoch - start_epoch\n",
    "    total_time += elapsed\n",
    "    \n",
    "print('The average time per inference for unquantized unet is')\n",
    "print(total_time/trials)\n",
    "\n",
    "\n",
    "## Training speed\n",
    "\n",
    "lr = .0001 # Doesn't matter\n",
    "lossfunc = nn.MSELoss().cuda('0')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "trials = 100\n",
    "total_time = 0\n",
    "batch_size = 1\n",
    "for i in range(trials):\n",
    "    torch.cuda.synchronize()\n",
    "                             \n",
    "    inp = torch.randn([batch_size, 3, 512, 512]).to(device)\n",
    "    label = torch.randn([batch_size, 1, 512, 512]).to(device)\n",
    "    start_epoch = time.time()\n",
    "                                                    \n",
    "    output = model(inp)                         \n",
    "    loss = lossfunc(output,label)\n",
    "    loss.backward()\n",
    "    optimizer.step()              \n",
    "                             \n",
    "    torch.cuda.synchronize()\n",
    "    end_epoch = time.time()\n",
    "    elapsed = end_epoch - start_epoch\n",
    "    total_time += elapsed\n",
    "    \n",
    "print('The average time per training cycle (batch_size = 1) for unquantized unet is')\n",
    "print(total_time/trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd4275a-15ce-4c19-92da-21ef026b1f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'fcn_resnet50', pretrained=False)\n",
    "# or\n",
    "device = 'cuda:0'\n",
    "model = model.to(device)\n",
    "inp = torch.randn([1,3,512,512]).to(device)\n",
    "\n",
    "# print(model(inp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16f3443-21a1-4159-904e-72448b2ca41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = .0001 # Doesn't matter\n",
    "\n",
    "import torch.nn as nn\n",
    "import time\n",
    "\n",
    "lossfunc = nn.MSELoss().cuda('0')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "trials = 100\n",
    "total_time = 0\n",
    "batch_size = 1\n",
    "for i in range(trials):\n",
    "    torch.cuda.synchronize()\n",
    "                             \n",
    "    inp = torch.randn([batch_size, 3, 512, 512]).to(device)\n",
    "    inp2 = torch.randn([batch_size, 1, 512, 512]).to(device)\n",
    "    \n",
    "    # Create artifical label\n",
    "\n",
    "    # label = torch.randn([batch_size, 1, 512, 512]).to(device)\n",
    "    start_epoch = time.time()\n",
    "                                                    \n",
    "    output = model(inp)                         \n",
    "    loss = lossfunc(output['out'],inp2)\n",
    "    loss.backward()\n",
    "    optimizer.step()              \n",
    "                             \n",
    "    torch.cuda.synchronize()\n",
    "    end_epoch = time.time()\n",
    "    elapsed = end_epoch - start_epoch\n",
    "    total_time += elapsed\n",
    "\n",
    "print('The average time per training cycle (batch_size = 1) for unquantized unet is')\n",
    "print(total_time/trials)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba0a8e0-2677-4d4b-b927-c9423f4fc609",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu110.m95",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m95"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
